{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# tensorboard --logdir=/opt/logs \n",
    "writer = SummaryWriter(log_dir=\"/opt/logs/quick\", flush_secs=30)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /opt/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:16<00:00, 10424687.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /opt/data/cifar-10-python.tar.gz to /opt/data\n",
      "Files already downloaded and verified\n",
      "训练样本总数: 50000\n",
      "测试样本总数: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR10(\n",
    "    root=\"/opt/data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"/opt/data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_size = len(train_data)\n",
    "valid_size = len(test_data)\n",
    "print(\"训练样本总数:\", train_size)\n",
    "print(\"测试样本总数:\", valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练批次数: 938\n",
      "测试批次数: 157\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print(\"训练批次数:\", len(train_dataloader))\n",
    "print(\"测试批次数:\", len(test_dataloader))\n",
    "for X, y in test_dataloader:\n",
    "    print(X.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # (1, 28, 28) -> (32, 26, 26)\n",
    "            # (32, 26, 26) -> (32, 13, 13)\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # (32, 13, 13) -> (64, 11, 11)\n",
    "            # (64, 11, 11) -> (64, 5, 5)\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 5 * 5, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "        # 权重初始化\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 0  # 总训练轮数\n",
    "best_accuracy = 0  # 最佳准确率\n",
    "model_path = \"/opt/models/quick.pth\"  # 模型保存路径\n",
    "\n",
    "initial_lr = 1e-2  # 初始学习率\n",
    "lr_patience = 10  # 学习率等待衰减次数\n",
    "lr_factor = 0.5  # 学习率衰减因子\n",
    "\n",
    "worse_count = 0  # 连续无增长计数\n",
    "worse_tolerance = 20  # 无增长容忍次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=lr_patience, factor=lr_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [04:19<01:46,  3.66s/it, accuracy=90.83%, best_accuracy=91.04%, lr=3e-04, valid_loss=1.02e-02, train_loss=1.41e-03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型连续20次无提升，提前终止训练\n",
      "模型架构: NeuralNetwork(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=1600, out_features=256, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.4, inplace=False)\n",
      "    (12): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "模型参数: 431434\n",
      "保存路径: /opt/quick.pth\n",
      "最佳训练轮数: 52\n",
      "最佳准确率: 91.03999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "pbar = tqdm(range(max_epochs))\n",
    "for i in pbar:\n",
    "    train_loss, valid_loss, accuracy = 0, 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    epoch_count += 1\n",
    "    train_loss /= train_size\n",
    "    valid_loss /= valid_size\n",
    "    accuracy = accuracy / valid_size * 100\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        worse_count = 0\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    else:\n",
    "        worse_count += 1\n",
    "        if worse_count == worse_tolerance:\n",
    "            print(f\"模型连续{worse_tolerance}次无提升，提前终止训练\")\n",
    "            break\n",
    "\n",
    "    writer.add_scalar(\"train_loss\", train_loss, i)\n",
    "    writer.add_scalar(\"valid_loss\", valid_loss, i)\n",
    "    writer.add_scalar(\"accuracy\", accuracy, i)\n",
    "    writer.add_scalar(\"learning_rate\", optimizer.param_groups[0][\"lr\"], i)\n",
    "    pbar.set_postfix(lr=f\"{optimizer.param_groups[0][\"lr\"]:.0e}\", train_loss=f\"{train_loss:.2e}\", valid_loss=f\"{valid_loss:.2e}\", accuracy=f\"{accuracy:.2f}%\", best_accuracy=f\"{best_accuracy:.2f}%\")\n",
    "\n",
    "print(\"模型架构:\", model)\n",
    "print(\"模型参数:\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"保存路径:\", model_path)\n",
    "print(\"最佳训练轮数:\", epoch_count - worse_tolerance)\n",
    "print(\"最佳准确率:\", best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
