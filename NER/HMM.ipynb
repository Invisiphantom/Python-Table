{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c32a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def estimate_hmm_parameters(train_file):\n",
    "    # 初始化统计变量\n",
    "    pi = defaultdict(int)  # 初始概率计数\n",
    "    A = defaultdict(lambda: defaultdict(int))  # 转移概率计数\n",
    "    B = defaultdict(lambda: defaultdict(int))  # 发射概率计数\n",
    "\n",
    "    vocab = set()  # 词汇表\n",
    "    tag_counts = defaultdict(int)  # 标签总出现次数\n",
    "    total_sentences = 0  # 总句子数\n",
    "\n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        current_sentence = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # 处理一个完整句子\n",
    "                if current_sentence:\n",
    "                    total_sentences += 1\n",
    "                    first_word, first_tag = current_sentence[0]\n",
    "                    pi[first_tag] += 1\n",
    "\n",
    "                    # 统计转移和发射概率\n",
    "                    for i in range(len(current_sentence)):\n",
    "                        word, tag = current_sentence[i]\n",
    "                        vocab.add(word)\n",
    "                        tag_counts[tag] += 1\n",
    "                        B[tag][word] += 1\n",
    "\n",
    "                        if i > 0:\n",
    "                            prev_tag = current_sentence[i - 1][1]\n",
    "                            A[prev_tag][tag] += 1\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                # 解析单词和标签\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    word, tag = parts[0].lower(), parts[1]\n",
    "                    current_sentence.append((word, tag))\n",
    "\n",
    "    # 计算概率（加一平滑）\n",
    "    vocab_size = len(vocab)\n",
    "    num_tags = len(tag_counts)\n",
    "\n",
    "    # 初始概率\n",
    "    pi_prob = {tag: (pi.get(tag, 0) + 1) / (total_sentences + num_tags) for tag in tag_counts}\n",
    "\n",
    "    # 转移概率\n",
    "    A_prob = {}\n",
    "    for prev_tag in tag_counts:\n",
    "        A_prob[prev_tag] = {}\n",
    "        total_transitions = sum(A[prev_tag].values())\n",
    "        for tag in tag_counts:\n",
    "            A_prob[prev_tag][tag] = (A[prev_tag].get(tag, 0) + 1) / (total_transitions + num_tags)\n",
    "\n",
    "    # 发射概率\n",
    "    B_prob = {}\n",
    "    for tag in tag_counts:\n",
    "        B_prob[tag] = {}\n",
    "        total_words = tag_counts[tag]\n",
    "        for word in vocab:\n",
    "            B_prob[tag][word] = (B[tag].get(word, 0) + 1) / (total_words + vocab_size)\n",
    "\n",
    "    return pi_prob, A_prob, B_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27231ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decode(obs_sequence, states, pi, A, B):\n",
    "    T = len(obs_sequence)\n",
    "    N = len(states)\n",
    "\n",
    "    delta = np.zeros((T, N))  # 最优概率\n",
    "    psi = np.zeros((T, N), dtype=int)  # 最优路径\n",
    "\n",
    "    # Step 1: 初始化首单词概率\n",
    "    for i, s in enumerate(states):\n",
    "        delta[0][i] = pi.get(s, 1e-6) * B.get(s, {}).get(obs_sequence[0], 1e-6)\n",
    "\n",
    "    # Step 2: 动态规划\n",
    "    for t in range(1, T):\n",
    "        for j, s in enumerate(states):\n",
    "            max_prob, best_prev = -1.0, 0\n",
    "            for i, prev_s in enumerate(states):\n",
    "                # 检查标签的合法性\n",
    "                if prev_s.startswith(\"B-\") and not (s.startswith(\"M-\") or s.startswith(\"E-\")):\n",
    "                    continue\n",
    "                if prev_s.startswith(\"M-\") and not (s.startswith(\"M-\") or s.startswith(\"E-\")):\n",
    "                    continue\n",
    "                if prev_s.startswith(\"E-\") and (s.startswith(\"M-\") or s.startswith(\"E-\")):\n",
    "                    continue\n",
    "\n",
    "                prob = delta[t - 1][i] * A.get(prev_s, {}).get(s, 1e-6) * B.get(s, {}).get(obs_sequence[t], 1e-6)\n",
    "\n",
    "                if prob > max_prob:\n",
    "                    max_prob, best_prev = prob, i\n",
    "\n",
    "            delta[t][j] = max_prob\n",
    "            psi[t][j] = best_prev\n",
    "\n",
    "    # Step 3: 回溯最优路径\n",
    "    best_path = [np.argmax(delta[-1])]\n",
    "    for t in range(T - 1, 0, -1):\n",
    "        best_path.insert(0, psi[t][best_path[0]])\n",
    "\n",
    "    return [states[i] for i in best_path]\n",
    "\n",
    "\n",
    "# 读取验证集并处理句子分隔\n",
    "def process_validation_file(input_file, output_file, states, pi, A, B):\n",
    "    current_sentence = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # 处理一个完整句子\n",
    "                if current_sentence:\n",
    "                    words = [word.lower() for word, _ in current_sentence]\n",
    "                    predicted_tags = viterbi_decode(words, states, pi, A, B)\n",
    "                    for (word, _), tag in zip(current_sentence, predicted_tags):\n",
    "                        fout.write(f\"{word} {tag}\\n\")\n",
    "                    fout.write(\"\\n\")\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                # 非空行，读取单词\n",
    "                parts = line.split()\n",
    "                word = parts[0]\n",
    "                current_sentence.append((word, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b279f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"O\", \"B-NAME\", \"M-NAME\", \"E-NAME\", \"S-NAME\", \"B-CONT\", \"M-CONT\", \"E-CONT\", \"S-CONT\", \"B-EDU\", \"M-EDU\", \"E-EDU\", \"S-EDU\", \"B-TITLE\", \"M-TITLE\", \"E-TITLE\", \"S-TITLE\", \"B-ORG\", \"M-ORG\", \"E-ORG\", \"S-ORG\", \"B-RACE\", \"M-RACE\", \"E-RACE\", \"S-RACE\", \"B-PRO\", \"M-PRO\", \"E-PRO\", \"S-PRO\", \"B-LOC\", \"M-LOC\", \"E-LOC\", \"S-LOC\"]\n",
    "pi, A, B = estimate_hmm_parameters(\"Chinese/train.txt\")\n",
    "process_validation_file(\"Chinese/validation.txt\", \"Chinese/validation_HMM.txt\", states, pi, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7353fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\n",
    "pi, A, B = estimate_hmm_parameters(\"English/train.txt\")\n",
    "process_validation_file(\"English/validation.txt\", \"English/validation_HMM.txt\", states, pi, A, B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
