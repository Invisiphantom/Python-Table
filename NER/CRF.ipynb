{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7087c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d8c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF:\n",
    "    def __init__(self, tags, feature_templates):\n",
    "        \"\"\"\n",
    "        :param feature_templates: 特征模板列表\n",
    "        :param tags: 标签列表\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.feature_templates = feature_templates\n",
    "        self.tag_to_idx = {tag: i for i, tag in enumerate(tags)}\n",
    "        self.idx_to_tag = {i: tag for tag, i in self.tag_to_idx.items()}\n",
    "\n",
    "        # 模型参数\n",
    "        self.weights = defaultdict(lambda: np.random.randn() * 0.01)  # 特征权重\n",
    "        self.transition = np.random.randn(len(tags), len(tags)) * 0.01  # 转移矩阵\n",
    "\n",
    "    def legal_transition(self, i, j):\n",
    "        i_tag = self.idx_to_tag[i]\n",
    "        j_tag = self.idx_to_tag[j]\n",
    "        if i_tag.startswith(\"B-\") and not (j_tag.startswith(\"M-\") or j_tag.startswith(\"E-\")):\n",
    "            return False\n",
    "        if i_tag.startswith(\"M-\") and not (j_tag.startswith(\"M-\") or j_tag.startswith(\"E-\")):\n",
    "            return False\n",
    "        if i_tag.startswith(\"E-\") and (j_tag.startswith(\"M-\") or j_tag.startswith(\"E-\")):\n",
    "            return False\n",
    "        if i_tag.startswith(\"S-\") and (j_tag.startswith(\"M-\") or j_tag.startswith(\"E-\")):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_features_helper(sentence, position, template):\n",
    "        \"\"\"提取特征模板中的偏移量\"\"\"\n",
    "        context = []  # U08:%x[0,0]/%x[1,0]\n",
    "        parts = template.split(\":\")[1].split(\"/\")\n",
    "        for part in parts:\n",
    "            offset = int(part[3:-1].split(\",\")[0])\n",
    "            idx = position + offset\n",
    "            if idx < 0:\n",
    "                context.append(\"<BEG>\")\n",
    "            elif idx >= len(sentence):\n",
    "                context.append(\"<END>\")\n",
    "            else:\n",
    "                context.append(sentence[idx])\n",
    "        return context\n",
    "\n",
    "    def extract_features(self, sentence, position, prev_tag, cur_tag):\n",
    "        \"\"\"根据特征模板提取特征\"\"\"\n",
    "        features = []\n",
    "        for template in self.feature_templates:\n",
    "            # 处理Unigram特征\n",
    "            if template.startswith(\"U\"):\n",
    "                context = self.extract_features_helper(sentence, position, template)\n",
    "                features.append(f\"{cur_tag}::{template}:{'/'.join(context)}\")\n",
    "\n",
    "            # 处理Bigram特征\n",
    "            elif template.startswith(\"B\"):\n",
    "                if prev_tag is not None:\n",
    "                    context = self.extract_features_helper(sentence, position, template)\n",
    "                    features.append(f\"{prev_tag}→{cur_tag}::{template}:{'/'.join(context)}\")\n",
    "        return features\n",
    "\n",
    "    def forward_backward(self, sentence):\n",
    "        \"\"\"前向后向算法\"\"\"\n",
    "        T = len(sentence)\n",
    "        N = len(self.tags)\n",
    "\n",
    "        # 初始化\n",
    "        alpha = np.zeros((T, N))\n",
    "        beta = np.zeros((T, N))\n",
    "\n",
    "        # 前向算法\n",
    "        # alpha_t(j)=\\sum_{i=1}^N alpha_{t-1}(i) * T(i,j) * E_t(j)\n",
    "        for t in range(T):\n",
    "            for j in range(N):\n",
    "                if t == 0:\n",
    "                    features = self.extract_features(sentence, t, None, self.idx_to_tag[j])\n",
    "                    alpha[t][j] = sum(self.weights[f] for f in features)\n",
    "                else:\n",
    "                    log_probs = []\n",
    "                    for i in range(N):\n",
    "                        trans = self.transition[i][j]  # T(i,j)\n",
    "                        features = self.extract_features(sentence, t, self.idx_to_tag[i], self.idx_to_tag[j])\n",
    "                        emit = sum(self.weights[f] for f in features)  # E_t(j)\n",
    "                        log_probs.append(alpha[t - 1][i] + trans + emit)\n",
    "                    alpha[t][j] = np.logaddexp.reduce(log_probs) if log_probs else -np.inf\n",
    "\n",
    "        # 后向算法\n",
    "        # beta_t(i)=\\sum_{j=1}^N T(i,j) * E_{t+1}(j) * beta_{t+1}(j)\n",
    "        for t in reversed(range(T)):\n",
    "            for i in range(N):\n",
    "                if t == T - 1:\n",
    "                    beta[t][i] = 0\n",
    "                else:\n",
    "                    log_probs = []\n",
    "                    for j in range(N):\n",
    "                        trans = self.transition[i][j]  # T(i,j)\n",
    "                        features = self.extract_features(sentence, t + 1, self.idx_to_tag[i], self.idx_to_tag[j])\n",
    "                        emit = sum(self.weights[f] for f in features)  # E_{t+1}(j)\n",
    "                        log_probs.append(trans + emit + beta[t + 1][j])\n",
    "                    beta[t][i] = np.logaddexp.reduce(log_probs) if log_probs else -np.inf\n",
    "\n",
    "        # 配分函数\n",
    "        # Z=\\sum_{j=1}^N alpha_T(j)\n",
    "        log_Z = np.log(sum(np.exp(alpha[-1]))) if any(np.isfinite(alpha[-1])) else -np.inf\n",
    "        return alpha, beta, log_Z\n",
    "\n",
    "    def compute_gradient(self, sentence, true_tags):\n",
    "        \"\"\"计算梯度\"\"\"\n",
    "        # 提取真实路径特征\n",
    "        true_features = set()\n",
    "        for t in range(len(sentence)):\n",
    "            prev_tag = true_tags[t - 1] if t > 0 else None\n",
    "            features = self.extract_features(sentence, t, prev_tag, true_tags[t])\n",
    "            true_features.update(features)\n",
    "\n",
    "        # 前向后向算法\n",
    "        alpha, beta, log_Z = self.forward_backward(sentence)\n",
    "        expected_features = defaultdict(float)\n",
    "\n",
    "        # 计算特征期望\n",
    "        # P(y_0=j|x)            = alpha_0(j) * E_0(j) / Z\n",
    "        # P(y_{t-1}=i, y_t=j|x) = alpha_{t-1}(i) * T(i,j) * E_t(j) * beta_t(j) / Z\n",
    "        for t in range(len(sentence)):\n",
    "            for i in range(len(self.tags)):\n",
    "                for j in range(len(self.tags)):\n",
    "                    # 提取特征\n",
    "                    cur_tag = self.idx_to_tag[j]\n",
    "                    prev_tag = self.idx_to_tag[i] if t > 0 else None\n",
    "                    features = self.extract_features(sentence, t, prev_tag, cur_tag)\n",
    "\n",
    "                    # 计算概率\n",
    "                    if t == 0:\n",
    "                        prob = np.exp(alpha[t][j] + beta[t][j] - log_Z) if log_Z != -np.inf else 0\n",
    "                    else:\n",
    "                        trans_score = self.transition[i][j]\n",
    "                        emit_score = sum(self.weights[f] for f in features)\n",
    "                        prob = np.exp(alpha[t - 1][i] + trans_score + emit_score + beta[t][j] - log_Z) if log_Z != -np.inf else 0\n",
    "\n",
    "                    # 累加特征期望\n",
    "                    for f in features:\n",
    "                        expected_features[f] += prob\n",
    "\n",
    "        # 计算权重梯度\n",
    "        # weight_grad[f] = true - expected\n",
    "        weight_grad = defaultdict(float)\n",
    "        for f in true_features:\n",
    "            weight_grad[f] += 1\n",
    "        for f in expected_features:\n",
    "            weight_grad[f] -= expected_features[f]\n",
    "\n",
    "        # 计算转移矩阵梯度\n",
    "        # transition_grad[f] = true - expected\n",
    "        transition_grad = np.zeros_like(self.transition)\n",
    "        for t in range(1, len(sentence)):\n",
    "            i = self.tag_to_idx[true_tags[t - 1]]\n",
    "            j = self.tag_to_idx[true_tags[t]]\n",
    "            transition_grad[i][j] += 1\n",
    "\n",
    "            for i_model in range(len(self.tags)):\n",
    "                for j_model in range(len(self.tags)):\n",
    "                    features = self.extract_features(sentence, t, self.idx_to_tag[i_model], self.idx_to_tag[j_model])\n",
    "                    prob = np.exp(alpha[t - 1][i_model] + self.transition[i_model][j_model] + sum(self.weights[f] for f in features) + beta[t][j_model] - log_Z) if log_Z != -np.inf else 0\n",
    "                    transition_grad[i_model][j_model] -= prob\n",
    "\n",
    "        return weight_grad, transition_grad, log_Z\n",
    "\n",
    "    def train(self, sentences, true_tags_seq, batch_size, max_iter, learning_rate):\n",
    "        for iteration in range(max_iter):\n",
    "            total_loss = 0\n",
    "            batch_indices = range(0, len(sentences), batch_size)\n",
    "\n",
    "            for start_idx in batch_indices:\n",
    "                end_idx = start_idx + batch_size\n",
    "                batch_sentences = sentences[start_idx:end_idx]\n",
    "                batch_tags = true_tags_seq[start_idx:end_idx]\n",
    "                print(f\"- 批次 {start_idx}-{end_idx}/{len(sentences)}\", end=\"  \")\n",
    "\n",
    "                # 初始化累积变量\n",
    "                batch_weights_grad = defaultdict(float)\n",
    "                batch_transition_grad = np.zeros_like(self.transition)\n",
    "                batch_loss = 0.0\n",
    "\n",
    "                # 计算批次内所有样本的梯度\n",
    "                for sentence, tags in zip(batch_sentences, batch_tags):\n",
    "                    # 计算单个样本的梯度\n",
    "                    weights_grad, transition_grad, log_Z = self.compute_gradient(sentence, tags)\n",
    "\n",
    "                    # 累积权重梯度\n",
    "                    for f in weights_grad:\n",
    "                        batch_weights_grad[f] += weights_grad[f]\n",
    "\n",
    "                    # 累积转移矩阵梯度\n",
    "                    batch_transition_grad += transition_grad\n",
    "\n",
    "                    # 计算单个样本的损失\n",
    "                    true_score = self._compute_single_score(sentence, tags)\n",
    "                    batch_loss += log_Z - true_score\n",
    "\n",
    "                # 计算批次平均梯度\n",
    "                batch_size_actual = len(batch_sentences)\n",
    "                for f in batch_weights_grad:\n",
    "                    batch_weights_grad[f] /= batch_size_actual\n",
    "                batch_transition_grad /= batch_size_actual\n",
    "                batch_loss /= batch_size_actual\n",
    "\n",
    "                # 使用平均梯度更新参数\n",
    "                for f in batch_weights_grad:\n",
    "                    self.weights[f] += learning_rate * batch_weights_grad[f]\n",
    "\n",
    "                for i in range(len(self.tags)):\n",
    "                    for j in range(len(self.tags)):\n",
    "                        if self.legal_transition(i, j):\n",
    "                            self.transition[i][j] += learning_rate * batch_transition_grad[i][j]\n",
    "\n",
    "                loss = batch_loss * batch_size_actual\n",
    "                print(f\"Loss={loss/batch_size_actual:.2f}\")\n",
    "                total_loss += loss\n",
    "\n",
    "            print(f\"迭代次数 {iteration}, Loss={total_loss/len(sentences):.2f}\")\n",
    "\n",
    "    def _compute_single_score(self, sentence, tags):\n",
    "        \"\"\"计算单个样本的真实路径得分\"\"\"\n",
    "        score = 0\n",
    "        for t in range(len(sentence)):\n",
    "            prev_tag = tags[t - 1] if t > 0 else None\n",
    "            features = self.extract_features(sentence, t, prev_tag, tags[t])\n",
    "            score += sum(self.weights[f] for f in features)\n",
    "            if t > 0:\n",
    "                i = self.tag_to_idx[tags[t - 1]]\n",
    "                j = self.tag_to_idx[tags[t]]\n",
    "                score += self.transition[i][j]\n",
    "        return score\n",
    "\n",
    "    def viterbi_decode(self, sentence):\n",
    "        T, N = len(sentence), len(self.tags)\n",
    "        viterbi = np.full((T, N), -np.inf)  # 初始为负无穷\n",
    "        backpointers = np.zeros((T, N), dtype=int)\n",
    "\n",
    "        # 初始步：开头仅允许B-,S-,O\n",
    "        for j in range(N):\n",
    "            tag = self.idx_to_tag[j]\n",
    "            if tag.startswith((\"B-\", \"S-\")) or tag == \"O\":\n",
    "                features = self.extract_features(sentence, 0, None, tag)\n",
    "                viterbi[0][j] = sum(self.weights[f] for f in features)\n",
    "\n",
    "        # 递推步：仅允许合法转移\n",
    "        for t in range(1, T):\n",
    "            for j in range(N):\n",
    "                max_score = -np.inf\n",
    "                best_i = -1\n",
    "                for i in range(N):\n",
    "                    if not self.legal_transition(i, j):  # 跳过非法转移\n",
    "                        continue\n",
    "                    score = viterbi[t - 1][i] + self.transition[i][j]\n",
    "                    features = self.extract_features(sentence, t, self.idx_to_tag[i], self.idx_to_tag[j])\n",
    "                    score += sum(self.weights[f] for f in features)\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        best_i = i\n",
    "                if best_i != -1:  # 确保存在合法前驱\n",
    "                    viterbi[t][j] = max_score\n",
    "                    backpointers[t][j] = best_i\n",
    "\n",
    "        # 回溯\n",
    "        best_path = [np.argmax(viterbi[-1])]\n",
    "        for t in reversed(range(1, T)):\n",
    "            best_path.append(backpointers[t][best_path[-1]])\n",
    "        best_path.reverse()\n",
    "\n",
    "        return [self.idx_to_tag[i] for i in best_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0ad175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练集\n",
    "def train_dataset(train_file):\n",
    "    train_sentences, train_tags = [], []\n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        cur_sentence = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if cur_sentence:\n",
    "                    train_sentences.append([word for word, _ in cur_sentence])\n",
    "                    train_tags.append([tag for _, tag in cur_sentence])\n",
    "                    cur_sentence = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                cur_sentence.append((parts[0], parts[1]))\n",
    "\n",
    "    return train_sentences, train_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51242cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_validation_file(input_file, output_file, crf):\n",
    "    current_sentence = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            if line.strip() == \"\":\n",
    "                # 处理一个完整句子\n",
    "                if current_sentence:\n",
    "                    words = [word.lower() for word, _ in current_sentence]\n",
    "                    predicted_tags = crf.viterbi_decode(words)\n",
    "                    for (word, _), tag in zip(current_sentence, predicted_tags):\n",
    "                        fout.write(f\"{word} {tag}\\n\")\n",
    "                    fout.write(\"\\n\")\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                # 非空行，读取单词\n",
    "                parts = line.split()\n",
    "                word = parts[0]\n",
    "                current_sentence.append((word, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693dd8f2",
   "metadata": {},
   "source": [
    "# Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30821cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新初始化CRF模型\n",
    "tags = [\"O\", \"B-NAME\", \"M-NAME\", \"E-NAME\", \"S-NAME\", \"B-CONT\", \"M-CONT\", \"E-CONT\", \"S-CONT\", \"B-EDU\", \"M-EDU\", \"E-EDU\", \"S-EDU\", \"B-TITLE\", \"M-TITLE\", \"E-TITLE\", \"S-TITLE\", \"B-ORG\", \"M-ORG\", \"E-ORG\", \"S-ORG\", \"B-RACE\", \"M-RACE\", \"E-RACE\", \"S-RACE\", \"B-PRO\", \"M-PRO\", \"E-PRO\", \"S-PRO\", \"B-LOC\", \"M-LOC\", \"E-LOC\", \"S-LOC\"]\n",
    "feature_templates = [\n",
    "    \"U01:%x[-1,0]\",  # 前一个词\n",
    "    \"U02:%x[0,0]\",  # 当前词\n",
    "]\n",
    "crf = CRF(tags, feature_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ba6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件加载CRF模型\n",
    "# with open(\"crf_Chinese.pkl\", \"rb\") as f:\n",
    "#     crf = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52964131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取中文训练集\n",
    "train_sentences, train_tags = train_dataset(\"Chinese/train_cut.txt\")\n",
    "# train_sentences, train_tags = train_dataset(\"Chinese/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a387a5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 批次 0-16/146  Loss=92.04\n",
      "- 批次 16-32/146  Loss=76.96\n",
      "- 批次 32-48/146  Loss=89.41\n",
      "- 批次 48-64/146  Loss=68.97\n",
      "- 批次 64-80/146  Loss=61.88\n",
      "- 批次 80-96/146  Loss=57.60\n",
      "- 批次 96-112/146  Loss=45.34\n",
      "- 批次 112-128/146  Loss=60.79\n",
      "- 批次 128-144/146  Loss=47.85\n",
      "- 批次 144-160/146  Loss=46.38\n",
      "迭代次数 0, Loss=66.48\n",
      "- 批次 0-16/146  Loss=39.17\n",
      "- 批次 16-32/146  Loss=29.97\n",
      "- 批次 32-48/146  Loss=37.48\n",
      "- 批次 48-64/146  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcrf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 185\u001b[39m, in \u001b[36mCRF.train\u001b[39m\u001b[34m(self, sentences, true_tags_seq, batch_size, max_iter, learning_rate)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# 计算批次内所有样本的梯度\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence, tags \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_sentences, batch_tags):\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# 计算单个样本的梯度\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     weights_grad, transition_grad, log_Z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# 累积权重梯度\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m weights_grad:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mCRF.compute_gradient\u001b[39m\u001b[34m(self, sentence, true_tags)\u001b[39m\n\u001b[32m    113\u001b[39m     true_features.update(features)\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# 前向后向算法\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m alpha, beta, log_Z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m expected_features = defaultdict(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# 计算特征期望\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# P(y_0=j|x)            = alpha_0(j) * E_0(j) / Z\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# P(y_{t-1}=i, y_t=j|x) = alpha_{t-1}(i) * T(i,j) * E_t(j) * beta_t(j) / Z\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mCRF.forward_backward\u001b[39m\u001b[34m(self, sentence)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m     95\u001b[39m     trans = \u001b[38;5;28mself\u001b[39m.transition[i][j]  \u001b[38;5;66;03m# T(i,j)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43midx_to_tag\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43midx_to_tag\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     emit = \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m.weights[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features)  \u001b[38;5;66;03m# E_{t+1}(j)\u001b[39;00m\n\u001b[32m     98\u001b[39m     log_probs.append(trans + emit + beta[t + \u001b[32m1\u001b[39m][j])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mCRF.extract_features\u001b[39m\u001b[34m(self, sentence, position, prev_tag, cur_tag)\u001b[39m\n\u001b[32m     42\u001b[39m             context.append(sentence[idx])\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence, position, prev_tag, cur_tag):\n\u001b[32m     46\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"根据特征模板提取特征\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m     features = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "crf.train(train_sentences, train_tags, batch_size=16, max_iter=5, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26130b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将CRF模型保存到文件\n",
    "dill.dump(crf, open(\"crf_Chinese.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试中文模型\n",
    "# process_validation_file(\"Chinese/validation.txt\", \"Chinese/validation_CRF.txt\", crf)\n",
    "process_validation_file(\"Chinese/chinese_test.txt\", \"Chinese/chinese_test_CRF.txt\", crf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1912230",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6956311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化CRF模型\n",
    "tags = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\n",
    "feature_templates = [\n",
    "    \"U01:%x[-1,0]\",  # 前一个词\n",
    "    \"U02:%x[0,0]\",  # 当前词\n",
    "]\n",
    "crf = CRF(tags, feature_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee045043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"crf_English.pkl\", \"rb\") as f:\n",
    "#     crf = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb389c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取英文训练集\n",
    "train_sentences, train_tags = train_dataset(\"English/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa543455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 批次 0-16/14041  Loss=33.96\n",
      "- 批次 16-32/14041  Loss=10.56\n",
      "- 批次 32-48/14041  Loss=12.75\n",
      "- 批次 48-64/14041  Loss=9.97\n",
      "- 批次 64-80/14041  Loss=13.24\n",
      "- 批次 80-96/14041  Loss=13.01\n",
      "- 批次 96-112/14041  Loss=9.49\n",
      "- 批次 112-128/14041  Loss=10.69\n",
      "- 批次 128-144/14041  Loss=8.86\n",
      "- 批次 144-160/14041  Loss=9.84\n",
      "- 批次 160-176/14041  Loss=12.38\n",
      "- 批次 176-192/14041  Loss=13.24\n",
      "- 批次 192-208/14041  Loss=9.12\n",
      "- 批次 208-224/14041  Loss=11.59\n",
      "- 批次 224-240/14041  Loss=11.36\n",
      "- 批次 240-256/14041  Loss=10.09\n",
      "- 批次 256-272/14041  Loss=6.54\n",
      "- 批次 272-288/14041  Loss=10.34\n",
      "- 批次 288-304/14041  Loss=11.37\n",
      "- 批次 304-320/14041  Loss=7.52\n",
      "- 批次 320-336/14041  Loss=8.78\n",
      "- 批次 336-352/14041  Loss=12.07\n",
      "- 批次 352-368/14041  Loss=11.75\n",
      "- 批次 368-384/14041  Loss=10.50\n",
      "- 批次 384-400/14041  Loss=14.12\n",
      "- 批次 400-416/14041  Loss=10.90\n",
      "- 批次 416-432/14041  Loss=9.62\n",
      "- 批次 432-448/14041  Loss=11.10\n",
      "- 批次 448-464/14041  Loss=15.23\n",
      "- 批次 464-480/14041  Loss=12.16\n",
      "- 批次 480-496/14041  Loss=7.61\n",
      "- 批次 496-512/14041  Loss=13.65\n",
      "- 批次 512-528/14041  Loss=7.36\n",
      "- 批次 528-544/14041  Loss=12.02\n",
      "- 批次 544-560/14041  Loss=7.06\n",
      "- 批次 560-576/14041  Loss=11.40\n",
      "- 批次 576-592/14041  Loss=10.46\n",
      "- 批次 592-608/14041  Loss=9.97\n",
      "- 批次 608-624/14041  Loss=18.95\n",
      "- 批次 624-640/14041  Loss=9.30\n",
      "- 批次 640-656/14041  Loss=10.83\n",
      "- 批次 656-672/14041  Loss=10.49\n",
      "- 批次 672-688/14041  Loss=8.85\n",
      "- 批次 688-704/14041  Loss=10.06\n",
      "- 批次 704-720/14041  Loss=11.29\n",
      "- 批次 720-736/14041  Loss=9.37\n",
      "- 批次 736-752/14041  Loss=14.41\n",
      "- 批次 752-768/14041  Loss=10.66\n",
      "- 批次 768-784/14041  Loss=10.41\n",
      "- 批次 784-800/14041  Loss=9.85\n",
      "- 批次 800-816/14041  Loss=7.13\n",
      "- 批次 816-832/14041  Loss=8.71\n",
      "- 批次 832-848/14041  Loss=9.47\n",
      "- 批次 848-864/14041  Loss=14.17\n",
      "- 批次 864-880/14041  Loss=13.43\n",
      "- 批次 880-896/14041  Loss=11.69\n",
      "- 批次 896-912/14041  Loss=11.47\n",
      "- 批次 912-928/14041  Loss=12.87\n",
      "- 批次 928-944/14041  Loss=11.17\n",
      "- 批次 944-960/14041  Loss=9.27\n",
      "- 批次 960-976/14041  Loss=7.08\n",
      "- 批次 976-992/14041  Loss=8.73\n",
      "- 批次 992-1008/14041  Loss=12.70\n",
      "- 批次 1008-1024/14041  Loss=11.62\n",
      "- 批次 1024-1040/14041  Loss=11.03\n",
      "- 批次 1040-1056/14041  Loss=10.98\n",
      "- 批次 1056-1072/14041  Loss=10.16\n",
      "- 批次 1072-1088/14041  Loss=12.62\n",
      "- 批次 1088-1104/14041  Loss=10.72\n",
      "- 批次 1104-1120/14041  Loss=10.76\n",
      "- 批次 1120-1136/14041  Loss=9.22\n",
      "- 批次 1136-1152/14041  Loss=12.04\n",
      "- 批次 1152-1168/14041  Loss=12.47\n",
      "- 批次 1168-1184/14041  Loss=10.78\n",
      "- 批次 1184-1200/14041  Loss=9.33\n",
      "- 批次 1200-1216/14041  Loss=11.18\n",
      "- 批次 1216-1232/14041  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcrf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 185\u001b[39m, in \u001b[36mCRF.train\u001b[39m\u001b[34m(self, sentences, true_tags_seq, batch_size, max_iter, learning_rate)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# 计算批次内所有样本的梯度\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence, tags \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_sentences, batch_tags):\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# 计算单个样本的梯度\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     weights_grad, transition_grad, log_Z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# 累积权重梯度\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m weights_grad:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 128\u001b[39m, in \u001b[36mCRF.compute_gradient\u001b[39m\u001b[34m(self, sentence, true_tags)\u001b[39m\n\u001b[32m    126\u001b[39m cur_tag = \u001b[38;5;28mself\u001b[39m.idx_to_tag[j]\n\u001b[32m    127\u001b[39m prev_tag = \u001b[38;5;28mself\u001b[39m.idx_to_tag[i] \u001b[38;5;28;01mif\u001b[39;00m t > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_tag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# 计算概率\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mCRF.extract_features\u001b[39m\u001b[34m(self, sentence, position, prev_tag, cur_tag)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_templates:\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# 处理Unigram特征\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m template.startswith(\u001b[33m\"\u001b[39m\u001b[33mU\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         context = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_features_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m         features.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcur_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m::\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m.join(context)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# 处理Bigram特征\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mCRF.extract_features_helper\u001b[39m\u001b[34m(sentence, position, template)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"提取特征模板中的偏移量\"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m context = []  \u001b[38;5;66;03m# U08:%x[0,0]/%x[1,0]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m parts = \u001b[43mtemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts:\n\u001b[32m     35\u001b[39m     offset = \u001b[38;5;28mint\u001b[39m(part[\u001b[32m3\u001b[39m:-\u001b[32m1\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "crf.train(train_sentences, train_tags, batch_size=16, max_iter=5, learning_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c35f8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将CRF模型保存到文件\n",
    "dill.dump(crf, open(\"crf_English.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b87bc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试英文模型\n",
    "# process_validation_file(\"English/validation.txt\", \"English/validation_CRF.txt\", crf)\n",
    "process_validation_file(\"English/english_test.txt\", \"English/english_test_CRF.txt\", crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6b805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
