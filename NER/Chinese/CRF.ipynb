{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.99\n",
      "Iteration 1, Loss: 0.18\n",
      "Iteration 2, Loss: 1.28\n",
      "Iteration 3, Loss: 1.10\n",
      "Iteration 4, Loss: 2.11\n",
      "Iteration 5, Loss: 1.01\n",
      "Iteration 6, Loss: 2.15\n",
      "Iteration 7, Loss: 1.73\n",
      "Iteration 8, Loss: 1.48\n",
      "Iteration 9, Loss: 2.28\n",
      "Iteration 10, Loss: 1.11\n",
      "Iteration 11, Loss: 2.35\n",
      "Iteration 12, Loss: 1.72\n",
      "Iteration 13, Loss: 1.61\n",
      "Iteration 14, Loss: 2.29\n",
      "Iteration 15, Loss: 1.14\n",
      "Iteration 16, Loss: 2.43\n",
      "Iteration 17, Loss: 1.66\n",
      "Iteration 18, Loss: 1.71\n",
      "Iteration 19, Loss: 2.27\n",
      "Iteration 20, Loss: 1.18\n",
      "Iteration 21, Loss: 2.47\n",
      "Iteration 22, Loss: 1.59\n",
      "Iteration 23, Loss: 1.80\n",
      "Iteration 24, Loss: 2.23\n",
      "Iteration 25, Loss: 1.23\n",
      "Iteration 26, Loss: 2.49\n",
      "Iteration 27, Loss: 1.51\n",
      "Iteration 28, Loss: 1.90\n",
      "Iteration 29, Loss: 2.18\n",
      "Iteration 30, Loss: 1.28\n",
      "Iteration 31, Loss: 2.49\n",
      "Iteration 32, Loss: 1.43\n",
      "Iteration 33, Loss: 1.99\n",
      "Iteration 34, Loss: 2.13\n",
      "Iteration 35, Loss: 1.34\n",
      "Iteration 36, Loss: 2.47\n",
      "Iteration 37, Loss: 1.35\n",
      "Iteration 38, Loss: 2.09\n",
      "Iteration 39, Loss: 2.07\n",
      "Iteration 40, Loss: 1.40\n",
      "Iteration 41, Loss: 2.44\n",
      "Iteration 42, Loss: 1.28\n",
      "Iteration 43, Loss: 2.18\n",
      "Iteration 44, Loss: 1.99\n",
      "Iteration 45, Loss: 1.46\n",
      "Iteration 46, Loss: 2.41\n",
      "Iteration 47, Loss: 1.22\n",
      "Iteration 48, Loss: 2.26\n",
      "Iteration 49, Loss: 1.91\n",
      "预测结果: [('王', 'I-NAME'), ('五', 'O'), ('来自', 'B-NAME'), ('上海', 'I-NAME')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class CRF:\n",
    "    def __init__(self, tags, feature_templates):\n",
    "        \"\"\"\n",
    "        :param feature_templates: 特征模板列表\n",
    "        :param tags: 标签列表\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.feature_templates = feature_templates\n",
    "        self.tag_to_idx = {tag: i for i, tag in enumerate(tags)}\n",
    "        self.idx_to_tag = {i: tag for tag, i in self.tag_to_idx.items()}\n",
    "\n",
    "        # 模型参数\n",
    "        self.weights = defaultdict(float)  # 特征权重\n",
    "        self.transition = np.random.randn(len(tags), len(tags)) * 0.01  # 转移矩阵\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_features_helper(sentence, position, template):\n",
    "        \"\"\"提取特征模板中的偏移量\"\"\"\n",
    "        context = []  # U08:%x[0,0]/%x[1,0]\n",
    "        parts = template.split(\":\")[1].split(\"/\")\n",
    "        for part in parts:\n",
    "            offset = int(part[3:-1].split(\",\")[0])\n",
    "            idx = position + offset\n",
    "            if idx < 0:\n",
    "                context.append(\"<BEG>\")\n",
    "            elif idx >= len(sentence):\n",
    "                context.append(\"<END>\")\n",
    "            else:\n",
    "                context.append(sentence[idx])\n",
    "        return context\n",
    "\n",
    "    def extract_features(self, sentence, position, prev_tag, cur_tag):\n",
    "        \"\"\"根据特征模板提取特征\"\"\"\n",
    "        features = []\n",
    "        for template in self.feature_templates:\n",
    "            # 处理Unigram特征\n",
    "            if template.startswith(\"U\"):\n",
    "                context = self.extract_features_helper(sentence, position, template)\n",
    "                features.append(f\"{cur_tag}::{template}:{'/'.join(context)}\")\n",
    "\n",
    "            # 处理Bigram特征\n",
    "            elif template.startswith(\"B\"):\n",
    "                if prev_tag is not None:\n",
    "                    context = self.extract_features_helper(sentence, position, template)\n",
    "                    features.append(f\"{prev_tag}→{cur_tag}::{template}:{'/'.join(context)}\")\n",
    "        return features\n",
    "\n",
    "    def forward_backward(self, sentence):\n",
    "        \"\"\"前向后向算法\"\"\"\n",
    "        T = len(sentence)\n",
    "        N = len(self.tags)\n",
    "\n",
    "        # 初始化\n",
    "        alpha = np.zeros((T, N))\n",
    "        beta = np.zeros((T, N))\n",
    "\n",
    "        # 前向算法\n",
    "        # alpha_t(j)=\\sum_{i=1}^N alpha_{t-1}(i) * T(i,j) * E_t(j)\n",
    "        for t in range(T):\n",
    "            for j in range(N):\n",
    "                if t == 0:\n",
    "                    features = self.extract_features(sentence, t, None, self.idx_to_tag[j])\n",
    "                    alpha[t][j] = sum(self.weights[f] for f in features)\n",
    "                else:\n",
    "                    log_probs = []\n",
    "                    for i in range(N):\n",
    "                        trans = self.transition[i][j]  # T(i,j)\n",
    "                        features = self.extract_features(sentence, t, self.idx_to_tag[i], self.idx_to_tag[j])\n",
    "                        emit = sum(self.weights[f] for f in features)  # E_t(j)\n",
    "                        log_probs.append(alpha[t - 1][i] + trans + emit)\n",
    "                    alpha[t][j] = np.logaddexp.reduce(log_probs) if log_probs else -np.inf\n",
    "\n",
    "        # 后向算法\n",
    "        # beta_t(i)=\\sum_{j=1}^N T(i,j) * E_{t+1}(j) * beta_{t+1}(j)\n",
    "        for t in reversed(range(T)):\n",
    "            for i in range(N):\n",
    "                if t == T - 1:\n",
    "                    beta[t][i] = 0\n",
    "                else:\n",
    "                    log_probs = []\n",
    "                    for j in range(N):\n",
    "                        trans = self.transition[i][j]  # T(i,j)\n",
    "                        features = self.extract_features(sentence, t + 1, self.idx_to_tag[i], self.idx_to_tag[j])\n",
    "                        emit = sum(self.weights[f] for f in features)  # E_{t+1}(j)\n",
    "                        log_probs.append(trans + emit + beta[t + 1][j])\n",
    "                    beta[t][i] = np.logaddexp.reduce(log_probs) if log_probs else -np.inf\n",
    "\n",
    "        # 配分函数\n",
    "        # Z=\\sum_{j=1}^N alpha_T(j)\n",
    "        log_Z = np.log(sum(np.exp(alpha[-1]))) if any(np.isfinite(alpha[-1])) else -np.inf\n",
    "        return alpha, beta, log_Z\n",
    "\n",
    "    def compute_gradient(self, sentence, true_tags):\n",
    "        \"\"\"计算梯度\"\"\"\n",
    "        # 提取真实路径特征\n",
    "        true_features = set()\n",
    "        for t in range(len(sentence)):\n",
    "            prev_tag = true_tags[t - 1] if t > 0 else None\n",
    "            features = self.extract_features(sentence, t, prev_tag, true_tags[t])\n",
    "            true_features.update(features)\n",
    "\n",
    "        # 前向后向算法\n",
    "        alpha, beta, log_Z = self.forward_backward(sentence)\n",
    "        expected_features = defaultdict(float)\n",
    "\n",
    "        # 计算特征期望\n",
    "        # P(y_0=j|x)            = alpha_0(j) * E_0(j) / Z\n",
    "        # P(y_{t-1}=i, y_t=j|x) = alpha_{t-1}(i) * T(i,j) * E_t(j) * beta_t(j) / Z\n",
    "        for t in range(len(sentence)):\n",
    "            for i in range(len(self.tags)):\n",
    "                for j in range(len(self.tags)):\n",
    "                    # 提取特征\n",
    "                    cur_tag = self.idx_to_tag[j]\n",
    "                    prev_tag = self.idx_to_tag[i] if t > 0 else None\n",
    "                    features = self.extract_features(sentence, t, prev_tag, cur_tag)\n",
    "\n",
    "                    # 计算概率\n",
    "                    if t == 0:\n",
    "                        prob = np.exp(alpha[t][j] + beta[t][j] - log_Z) if log_Z != -np.inf else 0\n",
    "                    else:\n",
    "                        trans_score = self.transition[i][j]\n",
    "                        emit_score = sum(self.weights[f] for f in features)\n",
    "                        prob = np.exp(alpha[t - 1][i] + trans_score + emit_score + beta[t][j] - log_Z) if log_Z != -np.inf else 0\n",
    "\n",
    "                    # 累加特征期望\n",
    "                    for f in features:\n",
    "                        expected_features[f] += prob\n",
    "\n",
    "        # 计算权重梯度\n",
    "        # weight_grad[f] = true - expected\n",
    "        weight_grad = defaultdict(float)\n",
    "        for f in true_features:\n",
    "            weight_grad[f] += 1\n",
    "        for f in expected_features:\n",
    "            weight_grad[f] -= expected_features[f]\n",
    "\n",
    "        # 计算转移矩阵梯度\n",
    "        # transition_grad[f] = true - expected\n",
    "        transition_grad = np.zeros_like(self.transition)\n",
    "        for t in range(1, len(sentence)):\n",
    "            i = self.tag_to_idx[true_tags[t - 1]]\n",
    "            j = self.tag_to_idx[true_tags[t]]\n",
    "            transition_grad[i][j] += 1\n",
    "\n",
    "            for i_model in range(len(self.tags)):\n",
    "                for j_model in range(len(self.tags)):\n",
    "                    features = self.extract_features(sentence, t, self.idx_to_tag[i_model], self.idx_to_tag[j_model])\n",
    "                    prob = np.exp(alpha[t - 1][i_model] + self.transition[i_model][j_model] + sum(self.weights[f] for f in features) + beta[t][j_model] - log_Z) if log_Z != -np.inf else 0\n",
    "                    transition_grad[i_model][j_model] -= prob\n",
    "\n",
    "        return weight_grad, transition_grad, log_Z\n",
    "\n",
    "    def train(self, sentences, true_tags_seq, max_iter=10, learning_rate=0.1):\n",
    "        for iteration in range(max_iter):\n",
    "            total_loss = 0\n",
    "            for sentence, tags in zip(sentences, true_tags_seq):\n",
    "                # 计算梯度\n",
    "                weights_grad, transition_grad, log_Z = self.compute_gradient(sentence, tags)\n",
    "\n",
    "                # 更新权重\n",
    "                for f in weights_grad:\n",
    "                    self.weights[f] += learning_rate * weights_grad[f]\n",
    "\n",
    "                # 更新转移矩阵\n",
    "                self.transition += learning_rate * transition_grad\n",
    "\n",
    "                # 计算真实路径得分\n",
    "                true_score = 0\n",
    "                for t in range(len(sentence)):\n",
    "                    prev_tag = tags[t - 1] if t > 0 else None\n",
    "                    features = self.extract_features(sentence, t, prev_tag, tags[t])\n",
    "                    true_score += sum(self.weights[f] for f in features)\n",
    "                    if t > 0:\n",
    "                        i = self.tag_to_idx[tags[t - 1]]\n",
    "                        j = self.tag_to_idx[tags[t]]\n",
    "                        true_score += self.transition[i][j]\n",
    "\n",
    "                # 累加损失\n",
    "                total_loss += log_Z - true_score\n",
    "\n",
    "            print(f\"Iteration {iteration}, Loss: {total_loss/len(sentences):.2f}\")\n",
    "\n",
    "    def viterbi_decode(self, sentence):\n",
    "        \"\"\"Viterbi算法解码\"\"\"\n",
    "        T = len(sentence)\n",
    "        N = len(self.tags)\n",
    "\n",
    "        # 初始化\n",
    "        viterbi = np.zeros((T, N))\n",
    "        backpointers = np.zeros((T, N), dtype=int)\n",
    "\n",
    "        # 初始步\n",
    "        for j in range(N):\n",
    "            features = self.extract_features(sentence, 0, None, self.idx_to_tag[j])\n",
    "            viterbi[0][j] = sum(self.weights[f] for f in features)\n",
    "\n",
    "        # 递推\n",
    "        for t in range(1, T):\n",
    "            for j in range(N):\n",
    "                max_score = -np.inf\n",
    "                best_tag = 0\n",
    "                for i in range(N):\n",
    "                    trans_score = self.transition[i][j]\n",
    "                    features = self.extract_features(sentence, t, self.idx_to_tag[i], self.idx_to_tag[j])\n",
    "                    emit_score = sum(self.weights[f] for f in features)\n",
    "                    score = viterbi[t - 1][i] + trans_score + emit_score\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        best_tag = i\n",
    "                viterbi[t][j] = max_score\n",
    "                backpointers[t][j] = best_tag\n",
    "\n",
    "        # 回溯\n",
    "        best_path = [np.argmax(viterbi[-1])]\n",
    "        for t in reversed(range(1, T)):\n",
    "            best_path.append(backpointers[t][best_path[-1]])\n",
    "        best_path.reverse()\n",
    "\n",
    "        return [self.idx_to_tag[i] for i in best_path]\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "# 定义特征模板\n",
    "feature_templates = [\n",
    "    \"U00:%x[-2,0]\",\n",
    "    \"U01:%x[-1,0]\",\n",
    "    \"U02:%x[0,0]\",\n",
    "    \"U03:%x[1,0]\",\n",
    "    \"U04:%x[2,0]\",\n",
    "    \"U05:%x[-2,0]/%x[-1,0]\",\n",
    "    \"U06:%x[-1,0]/%x[0,0]\",\n",
    "    \"U07:%x[-1,0]/%x[1,0]\",\n",
    "    \"U08:%x[0,0]/%x[1,0]\",\n",
    "    \"U09:%x[1,0]/%x[2,0]\",\n",
    "    \"B00:%x[-2,0]\",\n",
    "    \"B01:%x[-1,0]\",\n",
    "    \"B02:%x[0,0]\",\n",
    "    \"B03:%x[1,0]\",\n",
    "    \"B04:%x[2,0]\",\n",
    "    \"B05:%x[-2,0]/%x[-1,0]\",\n",
    "    \"B06:%x[-1,0]/%x[0,0]\",\n",
    "    \"B07:%x[-1,0]/%x[1,0]\",\n",
    "    \"B08:%x[0,0]/%x[1,0]\",\n",
    "    \"B09:%x[1,0]/%x[2,0]\",\n",
    "]\n",
    "\n",
    "# 定义标签集\n",
    "tags = [\"O\", \"B-NAME\", \"I-NAME\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "\n",
    "# 初始化CRF\n",
    "crf = CRF(tags, feature_templates)\n",
    "\n",
    "# 训练数据示例\n",
    "train_sentences = [[\"张\", \"三\", \"在\", \"北京\", \"工作\"], [\"李\", \"四\", \"是\", \"腾讯\", \"员工\"]]\n",
    "train_tags = [[\"B-NAME\", \"I-NAME\", \"O\", \"B-LOC\", \"O\"], [\"B-NAME\", \"I-NAME\", \"O\", \"B-ORG\", \"O\"]]\n",
    "\n",
    "# 训练模型\n",
    "crf.train(train_sentences, train_tags, max_iter=50)\n",
    "\n",
    "# 预测新句子\n",
    "test_sentence = [\"王\", \"五\", \"来自\", \"上海\"]\n",
    "predicted_tags = crf.viterbi_decode(test_sentence)\n",
    "print(\"预测结果:\", list(zip(test_sentence, predicted_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ad175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c97b964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf64e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6f6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6adf00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd6daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
