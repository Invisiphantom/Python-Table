{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d8c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 6.344877557594868\n",
      "Iteration 1, Loss: 4.212741980601052\n",
      "Iteration 2, Loss: 2.958379688950451\n",
      "Iteration 3, Loss: 2.5774879850168917\n",
      "Iteration 4, Loss: 2.465733598601348\n",
      "Iteration 5, Loss: 2.358909231774292\n",
      "Iteration 6, Loss: 2.3489671006052513\n",
      "Iteration 7, Loss: 2.3193085228320305\n",
      "Iteration 8, Loss: 2.307462776857801\n",
      "Iteration 9, Loss: 2.30196952852915\n",
      "Iteration 10, Loss: 2.2933017430975684\n",
      "Iteration 11, Loss: 2.2908562881073937\n",
      "Iteration 12, Loss: 2.2870125057730535\n",
      "Iteration 13, Loss: 2.284607761580781\n",
      "Iteration 14, Loss: 2.2828926062658046\n",
      "Iteration 15, Loss: 2.28114039884063\n",
      "Iteration 16, Loss: 2.2800066395394145\n",
      "Iteration 17, Loss: 2.278914452616242\n",
      "Iteration 18, Loss: 2.27804057008705\n",
      "Iteration 19, Loss: 2.2773135240320537\n",
      "Iteration 20, Loss: 2.2766628336976553\n",
      "Iteration 21, Loss: 2.2761230381940383\n",
      "Iteration 22, Loss: 2.2756417992257987\n",
      "Iteration 23, Loss: 2.275222947220656\n",
      "Iteration 24, Loss: 2.274854428011335\n",
      "Iteration 25, Loss: 2.2745252291664055\n",
      "Iteration 26, Loss: 2.2742330937799062\n",
      "Iteration 27, Loss: 2.273970271215667\n",
      "Iteration 28, Loss: 2.2737337082592513\n",
      "Iteration 29, Loss: 2.2735197663730897\n",
      "Iteration 30, Loss: 2.2733252941880977\n",
      "Iteration 31, Loss: 2.2731481392230517\n",
      "Iteration 32, Loss: 2.272986042355795\n",
      "Iteration 33, Loss: 2.272837325437422\n",
      "Iteration 34, Loss: 2.272700472359679\n",
      "Iteration 35, Loss: 2.272574170015071\n",
      "Iteration 36, Loss: 2.2724573207980754\n",
      "Iteration 37, Loss: 2.2723489397289747\n",
      "Iteration 38, Loss: 2.2722481861086816\n",
      "Iteration 39, Loss: 2.2721543184367246\n",
      "Iteration 40, Loss: 2.272066684588424\n",
      "Iteration 41, Loss: 2.2719847124148753\n",
      "Iteration 42, Loss: 2.271907893440005\n",
      "Iteration 43, Loss: 2.271835777735692\n",
      "Iteration 44, Loss: 2.2717679643943924\n",
      "Iteration 45, Loss: 2.271704095456311\n",
      "Iteration 46, Loss: 2.2716438506811905\n",
      "Iteration 47, Loss: 2.2715869424593507\n",
      "Iteration 48, Loss: 2.2715331120847964\n",
      "Iteration 49, Loss: 2.2714821261740035\n",
      "预测结果: [('王', 'I-NAME'), ('五', 'I-NAME'), ('来自', 'O'), ('上海', 'O')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class CRF:\n",
    "    def __init__(self, feature_templates, tags):\n",
    "        \"\"\"\n",
    "        :param feature_templates: 特征模板列表\n",
    "        :param tags: 标签列表\n",
    "        \"\"\"\n",
    "        self.feature_templates = feature_templates\n",
    "        self.tags = tags\n",
    "        self.tag_to_idx = {tag: i for i, tag in enumerate(tags)}\n",
    "        self.idx_to_tag = {i: tag for tag, i in self.tag_to_idx.items()}\n",
    "\n",
    "        # 模型参数\n",
    "        self.weights = defaultdict(float)  # 特征权重\n",
    "        self.transition = np.zeros((len(tags), len(tags)))  # 转移矩阵\n",
    "\n",
    "    def extract_features(self, sentence, position, prev_tag, current_tag):\n",
    "        \"\"\"根据特征模板提取特征\"\"\"\n",
    "        features = []\n",
    "        for template in self.feature_templates:\n",
    "            # 处理Unigram特征 U08:%x[0,0]/%x[1,0]\n",
    "            if template.startswith(\"U\"):\n",
    "                parts = template.split(\":\")[1].split(\"/\")\n",
    "                context = []\n",
    "                for part in parts:\n",
    "                    offset = int(part[3:-1].split(\",\")[0])\n",
    "                    idx = position + offset\n",
    "\n",
    "                    if idx < 0:\n",
    "                        context.append(\"<BEG>\")\n",
    "                    elif idx >= len(sentence):\n",
    "                        context.append(\"<END>\")\n",
    "                    else:\n",
    "                        context.append(sentence[idx])\n",
    "                features.append(f\"{current_tag}::{template}:{'/'.join(context)}\")\n",
    "\n",
    "            # 处理Bigram特征 B08:%x[0,0]/%x[1,0]\n",
    "            elif template.startswith(\"B\"):\n",
    "                if prev_tag is not None:\n",
    "                    parts = template.split(\":\")[1].split(\"/\")\n",
    "                    context = []\n",
    "                    for part in parts:\n",
    "                        offset = int(part[3:-1].split(\",\")[0])\n",
    "                        idx = position + offset\n",
    "                        if idx < 0:\n",
    "                            context.append(\"<BEG>\")\n",
    "                        elif idx >= len(sentence):\n",
    "                            context.append(\"<END>\")\n",
    "                        else:\n",
    "                            context.append(sentence[idx])\n",
    "                    features.append(f\"{prev_tag}→{current_tag}::{template}:{'/'.join(context)}\")\n",
    "        return features\n",
    "\n",
    "    def forward_backward(self, sentence):\n",
    "        \"\"\"\n",
    "        前向-后向算法计算特征期望\n",
    "        :param sentence: 输入句子（单词列表）\n",
    "        :return: (alpha, beta, log_Z)\n",
    "        \"\"\"\n",
    "        T = len(sentence)\n",
    "        N = len(self.tags)\n",
    "        \n",
    "        # 初始化\n",
    "        alpha = np.zeros((T, N))\n",
    "        beta = np.zeros((T, N))\n",
    "        \n",
    "        # 前向算法\n",
    "        for t in range(T):\n",
    "            for j in range(N):\n",
    "                if t == 0:\n",
    "                    # 初始状态\n",
    "                    features = self.extract_features(sentence, t, None, self.idx_to_tag[j])\n",
    "                    score = sum(self.weights[f] for f in features)\n",
    "                    alpha[t][j] = score\n",
    "                else:\n",
    "                    sum_exp = 0\n",
    "                    for i in range(N):\n",
    "                        trans_score = self.transition[i][j]\n",
    "                        features = self.extract_features(sentence, t, self.idx_to_tag[i], self.idx_to_tag[j])\n",
    "                        emit_score = sum(self.weights[f] for f in features)\n",
    "                        sum_exp += np.exp(alpha[t-1][i] + trans_score + emit_score)\n",
    "                    alpha[t][j] = np.log(sum_exp) if sum_exp > 0 else -np.inf\n",
    "        \n",
    "        # 后向算法\n",
    "        for t in reversed(range(T)):\n",
    "            for i in range(N):\n",
    "                if t == T - 1:\n",
    "                    beta[t][i] = 0  # 终止状态\n",
    "                else:\n",
    "                    sum_exp = 0\n",
    "                    for j in range(N):\n",
    "                        trans_score = self.transition[i][j]\n",
    "                        features = self.extract_features(sentence, t+1, self.idx_to_tag[i], self.idx_to_tag[j])\n",
    "                        emit_score = sum(self.weights[f] for f in features)\n",
    "                        sum_exp += np.exp(trans_score + emit_score + beta[t+1][j])\n",
    "                    beta[t][i] = np.log(sum_exp) if sum_exp > 0 else -np.inf\n",
    "        \n",
    "        # 计算配分函数\n",
    "        log_Z = np.log(sum(np.exp(alpha[-1]))) if any(alpha[-1] != -np.inf) else -np.inf\n",
    "        \n",
    "        return alpha, beta, log_Z\n",
    "\n",
    "    def compute_gradient(self, sentence, true_tags):\n",
    "        \"\"\"计算梯度\"\"\"\n",
    "        # 提取真实路径特征\n",
    "        true_features = set()\n",
    "        for t in range(len(sentence)):\n",
    "            prev_tag = true_tags[t-1] if t > 0 else None\n",
    "            features = self.extract_features(sentence, t, prev_tag, true_tags[t])\n",
    "            true_features.update(features)\n",
    "        \n",
    "        # 计算模型期望特征（修正：只传入sentence）\n",
    "        alpha, beta, log_Z = self.forward_backward(sentence)\n",
    "        T = len(sentence)\n",
    "        expected_features = defaultdict(float)\n",
    "        \n",
    "        for t in range(T):\n",
    "            for i in range(len(self.tags)):\n",
    "                for j in range(len(self.tags)):\n",
    "                    if t == 0:\n",
    "                        prev_tag = None\n",
    "                    else:\n",
    "                        prev_tag = self.idx_to_tag[i]\n",
    "                    \n",
    "                    current_tag = self.idx_to_tag[j]\n",
    "                    features = self.extract_features(sentence, t, prev_tag, current_tag)\n",
    "                    \n",
    "                    # 计算概率\n",
    "                    if t == 0:\n",
    "                        prob = np.exp(alpha[t][j] + beta[t][j] - log_Z) if log_Z != -np.inf else 0\n",
    "                    else:\n",
    "                        trans_score = self.transition[i][j]\n",
    "                        emit_score = sum(self.weights[f] for f in features)\n",
    "                        prob = np.exp(alpha[t-1][i] + trans_score + emit_score + beta[t][j] - log_Z) if log_Z != -np.inf else 0\n",
    "                    \n",
    "                    # 累加特征期望\n",
    "                    for f in features:\n",
    "                        expected_features[f] += prob\n",
    "        \n",
    "        # 计算梯度\n",
    "        gradient = defaultdict(float)\n",
    "        for f in true_features:\n",
    "            gradient[f] += 1  # 真实特征计数\n",
    "        for f in expected_features:\n",
    "            gradient[f] -= expected_features[f]  # 模型期望\n",
    "        \n",
    "        return gradient, log_Z\n",
    "\n",
    "    def train(self, sentences, true_tag_sequences, max_iter=10, learning_rate=0.1):\n",
    "        \"\"\"训练CRF模型\"\"\"\n",
    "        for iteration in range(max_iter):\n",
    "            total_loss = 0\n",
    "            for sentence, tags in zip(sentences, true_tag_sequences):\n",
    "                # 计算梯度和损失\n",
    "                gradient, log_Z = self.compute_gradient(sentence, tags)\n",
    "\n",
    "                # 更新权重\n",
    "                for f in gradient:\n",
    "                    self.weights[f] += learning_rate * gradient[f]\n",
    "\n",
    "                # 计算真实路径得分\n",
    "                true_score = 0\n",
    "                for t in range(len(sentence)):\n",
    "                    prev_tag = tags[t - 1] if t > 0 else None\n",
    "                    features = self.extract_features(sentence, t, prev_tag, tags[t])\n",
    "                    true_score += sum(self.weights[f] for f in features)\n",
    "                    if t > 0:\n",
    "                        i = self.tag_to_idx[tags[t - 1]]\n",
    "                        j = self.tag_to_idx[tags[t]]\n",
    "                        true_score += self.transition[i][j]\n",
    "\n",
    "                # 累加损失\n",
    "                total_loss += log_Z - true_score\n",
    "\n",
    "            print(f\"Iteration {iteration}, Loss: {total_loss/len(sentences)}\")\n",
    "\n",
    "    def viterbi_decode(self, sentence):\n",
    "        \"\"\"Viterbi算法解码\"\"\"\n",
    "        T = len(sentence)\n",
    "        N = len(self.tags)\n",
    "\n",
    "        # 初始化\n",
    "        viterbi = np.zeros((T, N))\n",
    "        backpointers = np.zeros((T, N), dtype=int)\n",
    "\n",
    "        # 初始步\n",
    "        for j in range(N):\n",
    "            features = self.extract_features(sentence, 0, None, self.idx_to_tag[j])\n",
    "            viterbi[0][j] = sum(self.weights[f] for f in features)\n",
    "\n",
    "        # 递推\n",
    "        for t in range(1, T):\n",
    "            for j in range(N):\n",
    "                max_score = -np.inf\n",
    "                best_tag = 0\n",
    "                for i in range(N):\n",
    "                    trans_score = self.transition[i][j]\n",
    "                    features = self.extract_features(sentence, t, self.idx_to_tag[i], self.idx_to_tag[j])\n",
    "                    emit_score = sum(self.weights[f] for f in features)\n",
    "                    score = viterbi[t - 1][i] + trans_score + emit_score\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        best_tag = i\n",
    "                viterbi[t][j] = max_score\n",
    "                backpointers[t][j] = best_tag\n",
    "\n",
    "        # 回溯\n",
    "        best_path = [np.argmax(viterbi[-1])]\n",
    "        for t in reversed(range(1, T)):\n",
    "            best_path.append(backpointers[t][best_path[-1]])\n",
    "        best_path.reverse()\n",
    "\n",
    "        return [self.idx_to_tag[i] for i in best_path]\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "# 定义特征模板\n",
    "feature_templates = [\n",
    "    \"U00:%x[-2,0]\",\n",
    "    \"U01:%x[-1,0]\",\n",
    "    \"U02:%x[0,0]\",\n",
    "    \"U03:%x[1,0]\",\n",
    "    \"U04:%x[2,0]\",\n",
    "    \"U05:%x[-2,0]/%x[-1,0]\",\n",
    "    \"U06:%x[-1,0]/%x[0,0]\",\n",
    "    \"U07:%x[-1,0]/%x[1,0]\",\n",
    "    \"U08:%x[0,0]/%x[1,0]\",\n",
    "    \"U09:%x[1,0]/%x[2,0]\",\n",
    "]\n",
    "\n",
    "# 定义标签集\n",
    "tags = [\"O\", \"B-NAME\", \"I-NAME\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "\n",
    "# 初始化CRF\n",
    "crf = CRF(feature_templates, tags)\n",
    "\n",
    "# 训练数据示例\n",
    "train_sentences = [[\"张\", \"三\", \"在\", \"北京\", \"工作\"], [\"李\", \"四\", \"是\", \"腾讯\", \"员工\"]]\n",
    "train_tags = [[\"B-NAME\", \"I-NAME\", \"O\", \"B-LOC\", \"O\"], [\"B-NAME\", \"I-NAME\", \"O\", \"B-ORG\", \"O\"]]\n",
    "\n",
    "# 训练模型\n",
    "crf.train(train_sentences, train_tags, max_iter=50)\n",
    "\n",
    "# 预测新句子\n",
    "test_sentence = [\"王\", \"五\", \"来自\", \"上海\"]\n",
    "predicted_tags = crf.viterbi_decode(test_sentence)\n",
    "print(\"预测结果:\", list(zip(test_sentence, predicted_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ad175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c97b964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf64e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6f6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6adf00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd6daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
