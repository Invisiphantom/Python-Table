{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! 暂时只考虑坏地的情况\n",
    "A_year_len = 8\n",
    "A_land_len = 26\n",
    "A_crop_len = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 耕地性质, 作物性质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_df = pd.read_excel(\"耕地性质.xlsx\")\n",
    "crop_df = pd.read_excel(\"农作物性质.xlsx\")\n",
    "\n",
    "land_name2idx = {}\n",
    "land_idx2name = {}\n",
    "land_idx2type = {}\n",
    "land_idx2area = {}\n",
    "for i in range(len(land_df)):\n",
    "    land_name = land_df.iloc[i]['地块名称'].strip()\n",
    "    land_type = land_df.iloc[i]['地块类型'].strip()\n",
    "    land_area = land_df.iloc[i]['地块面积/亩']\n",
    "    land_name2idx[land_name] = i\n",
    "    land_idx2name[i] = land_name\n",
    "    land_idx2type[i] = land_type\n",
    "    land_idx2area[i] = land_area\n",
    "    \n",
    "crop_name2idx = {}\n",
    "crop_idx2name = {}\n",
    "crop_idx2type = {}\n",
    "crop_idx2land = {}\n",
    "for i in range(len(crop_df)):\n",
    "    crop_name = crop_df.iloc[i]['作物名称'].strip()\n",
    "    crop_type = crop_df.iloc[i]['作物类型'].strip()\n",
    "    crop_land = crop_df.iloc[i]['种植耕地'].strip()\n",
    "    crop_name2idx[crop_name] = i\n",
    "    crop_idx2name[i] = crop_name\n",
    "    crop_idx2type[i] = crop_type\n",
    "    crop_idx2land[i] = crop_land\n",
    "\n",
    "del land_df, crop_df, i\n",
    "del land_name, land_type, land_area, crop_name, crop_type, crop_land"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 销售价格 `price_ts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_2023 = pd.read_excel(\"2023年销售数据.xlsx\")\n",
    "\n",
    "# 计算每种作物的每亩均值利润\n",
    "for i in range(len(price_2023)):\n",
    "    price_range = price_2023.loc[i, \"销售单价/(元/斤)\"]\n",
    "    price_low = price_range.split(\"-\")[0]\n",
    "    price_high = price_range.split(\"-\")[1]\n",
    "    price_avg = (float(price_low) + float(price_high)) / 2\n",
    "    price_2023.loc[i, \"均值单价\"] = round(price_avg, 4)\n",
    "    price_2023.loc[i, \"均值利润\"] = price_2023.loc[i, \"亩产量/斤\"] * price_avg - price_2023.loc[i, \"种植成本/(元/亩)\"]\n",
    "    price_2023.loc[i, \"均值利润\"] = price_2023.loc[i, \"均值利润\"].round(4)\n",
    "\n",
    "del price_range, price_low, price_high, price_avg, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# 建立数据库 <作物索引, 地块类型, 季次, 每亩利润>\n",
    "conn = sqlite3.connect(\"price.db\")\n",
    "price_db = conn.cursor()\n",
    "price_db.execute(\"DROP TABLE IF EXISTS price_tb\")\n",
    "price_db.execute(\"CREATE TABLE IF NOT EXISTS price_tb (crop_idx INTEGER, land_type TEXT, season TEXT, price REAL)\")\n",
    "for i in range(len(price_2023)):\n",
    "    crop_name = price_2023.loc[i, \"作物名称\"].strip()\n",
    "    crop_idx = crop_name2idx[crop_name]\n",
    "    land_type = price_2023.loc[i, \"地块类型\"].strip()\n",
    "    season = price_2023.loc[i, \"种植季次\"].strip()\n",
    "    price = price_2023.loc[i, \"均值利润\"]\n",
    "    price_db.execute(\"INSERT INTO price_tb VALUES (?, ?, ?, ?)\", (crop_idx, land_type, season, price))\n",
    "    if land_type == \"普通大棚\":\n",
    "        price_db.execute(\"INSERT INTO price_tb VALUES (?, ?, ?, ?)\", (crop_idx, \"智慧大棚\", season, price))\n",
    "\n",
    "conn.commit()\n",
    "del crop_name, crop_idx, land_type, season, price, i, conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建销售利润价格矩阵 <地块索引, 作物索引>-<价格利润>\n",
    "price_ts = torch.zeros(A_land_len, A_crop_len)\n",
    "for i in range(A_land_len):\n",
    "    land_type = land_idx2type[i]\n",
    "    for j in range(A_crop_len):\n",
    "        price_db.execute(\"SELECT price FROM price_tb WHERE crop_idx = ? AND land_type = ?\", (j, land_type)) #! 暂时只考虑单季\n",
    "        crop_price = price_db.fetchone()\n",
    "        if crop_price is not None:\n",
    "            price_ts[i, j] = crop_price[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 耕地面积限制 `capat_ts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "capat_ts = torch.tensor([land_idx2area[i] for i in range(A_land_len)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作物销量 `demand_ts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设需求量是23年产量的SALE2VOL倍\n",
    "SALE2VOL = 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_2023 = pd.read_excel(\"2023年种植数据.xlsx\")\n",
    "demand_ts = torch.zeros(A_crop_len)\n",
    "for i in range(len(plant_2023)):\n",
    "    crop_idx = crop_name2idx[plant_2023.loc[i, \"作物名称\"].strip()]\n",
    "    crop_sale = plant_2023.loc[i, \"种植面积/亩\"]\n",
    "    if crop_idx < A_crop_len:\n",
    "        demand_ts[crop_idx] += crop_sale * SALE2VOL\n",
    "\n",
    "del plant_2023, crop_idx, crop_sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfitModel(nn.Module):\n",
    "    def __init__(self, year_len: int, land_len: int, crop_len: int, price_ts: torch.Tensor, capat_ts: torch.Tensor, demand_ts: torch.Tensor, device: str, focus_mask: float):\n",
    "        super(ProfitModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.land_len = land_len\n",
    "        self.crop_len = crop_len\n",
    "        self.plant_ts = nn.Parameter(torch.zeros(land_len, crop_len).to(torch.float32))  # 每块地的种植面积需要训练\n",
    "\n",
    "        assert price_ts.shape == (land_len, crop_len)\n",
    "        self.price_ts = price_ts.transpose(0, 1).to(device)  # 销售价格已确定, 无需训练\n",
    "\n",
    "        assert capat_ts.shape == (land_len,)\n",
    "        self.capat_ts = capat_ts.to(device)  # 每块地的种植面积上限已确定, 无需训练\n",
    "        self.plant_mask = (self.capat_ts * focus_mask).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "        assert demand_ts.shape == (crop_len,)\n",
    "        self.demand_ts = demand_ts.to(device)  # 每种作物的需求量已确定, 无需训练\n",
    "\n",
    "    def forward(self):\n",
    "        # 如果plant_ts的种植过于分散, 小于阈值则清零\n",
    "        torch.where(self.plant_ts < self.plant_mask, -self.plant_ts, self.plant_ts)\n",
    "        # 矩阵相乘, 并取对角线元素之和\n",
    "        profit_ts = torch.matmul(self.plant_ts, self.price_ts)\n",
    "        profit_sum = profit_ts.diag().sum()\n",
    "        # 如果某块耕地的种植面积超过了上限, 则惩罚\n",
    "        profit_sum -= torch.relu(self.plant_ts.sum(dim=1) - self.capat_ts).sum() * 1e9\n",
    "        # 如果某种作物的产量超过了需求量, 则惩罚\n",
    "        profit_sum -= torch.relu(self.plant_ts.transpose(0, 1).sum(dim=1) - self.demand_ts).sum() * 1e9\n",
    "        return -profit_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "focus_mask = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = ProfitModel(A_year_len, A_land_len, A_crop_len, price_ts, capat_ts, demand_ts, device, focus_mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_vars = [\"model\", \"crop_idx2name\", \"A_crop_len\", \"land_idx2name\", \"A_land_len\"]\n",
    "for var in list(globals().keys()):\n",
    "    if var not in keep_vars and not var.startswith(\"_\"):\n",
    "        del globals()[var]\n",
    "del var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 2188035.5\n",
      "Epoch 100, Loss 2194996.0\n",
      "Epoch 200, Loss 2204871.0\n",
      "Epoch 300, Loss 2214747.25\n",
      "Epoch 400, Loss 2224624.0\n",
      "Epoch 500, Loss 2234501.0\n",
      "Epoch 600, Loss 2242999.5\n",
      "Epoch 700, Loss 2251921.75\n",
      "Epoch 800, Loss 2260845.0\n",
      "Epoch 900, Loss 2269769.5\n",
      "Epoch 1000, Loss 2278695.0\n",
      "Epoch 1100, Loss 2287621.5\n",
      "Epoch 1200, Loss 2296548.75\n",
      "Epoch 1300, Loss 2305476.25\n",
      "Epoch 1400, Loss 2314405.5\n",
      "Epoch 1500, Loss 2323336.5\n",
      "Epoch 1600, Loss 2326778.75\n",
      "Epoch 1700, Loss 2332715.0\n",
      "Epoch 1800, Loss 2338653.0\n",
      "Epoch 1900, Loss 2344591.75\n",
      "Epoch 2000, Loss 2350531.25\n",
      "Epoch 2100, Loss 2356474.5\n",
      "Epoch 2200, Loss 2362419.0\n",
      "Epoch 2300, Loss 2368364.0\n",
      "Epoch 2400, Loss 2374310.5\n",
      "Epoch 2500, Loss 2377912.75\n",
      "Epoch 2600, Loss 2379669.25\n",
      "Epoch 2700, Loss 2382860.5\n",
      "Epoch 2800, Loss 2386070.75\n",
      "Epoch 2900, Loss 2389282.0\n",
      "Epoch 3000, Loss 2392494.75\n",
      "Epoch 3100, Loss 2395708.5\n",
      "Epoch 3200, Loss 2398923.5\n",
      "Epoch 3300, Loss 2402140.0\n",
      "Epoch 3400, Loss 2405358.75\n",
      "Epoch 3500, Loss 2408578.5\n",
      "Epoch 3600, Loss 2411800.5\n",
      "Epoch 3700, Loss 2415023.75\n",
      "Epoch 3800, Loss 2418249.25\n",
      "Epoch 3900, Loss 2421476.25\n",
      "Epoch 4000, Loss 2424705.75\n",
      "Epoch 4100, Loss 2427937.0\n",
      "Epoch 4200, Loss 2431170.25\n",
      "Epoch 4300, Loss 2434406.5\n",
      "Epoch 4400, Loss 2437645.5\n",
      "Epoch 4500, Loss 2440887.0\n",
      "Epoch 4600, Loss 2442389.75\n",
      "Epoch 4700, Loss 2444819.75\n",
      "Epoch 4800, Loss 2447258.0\n",
      "Epoch 4900, Loss 2449698.75\n",
      "Epoch 5000, Loss 2452143.0\n",
      "Epoch 5100, Loss 2454590.25\n",
      "Epoch 5200, Loss 2457040.75\n",
      "Epoch 5300, Loss 2459496.0\n",
      "Epoch 5400, Loss 2461954.5\n",
      "Epoch 5500, Loss 2464417.75\n",
      "Epoch 5600, Loss 2465106.0\n",
      "Epoch 5700, Loss 2466759.5\n",
      "Epoch 5800, Loss 2468422.0\n",
      "Epoch 5900, Loss 2470089.0\n",
      "Epoch 6000, Loss 2471761.5\n",
      "Epoch 6100, Loss 2473438.5\n",
      "Epoch 6200, Loss 2475121.5\n",
      "Epoch 6300, Loss 2476810.75\n",
      "Epoch 6400, Loss 2478506.5\n",
      "Epoch 6500, Loss 2480209.75\n",
      "Epoch 6600, Loss 2481919.75\n",
      "Epoch 6700, Loss 2483636.0\n",
      "Epoch 6800, Loss 2485359.75\n",
      "Epoch 6900, Loss 2487091.25\n",
      "Epoch 7000, Loss 2488831.5\n",
      "Epoch 7100, Loss 2490580.0\n",
      "Epoch 7200, Loss 2492338.0\n",
      "Epoch 7300, Loss 2494105.5\n",
      "Epoch 7400, Loss 2495883.5\n",
      "Epoch 7500, Loss 2497672.0\n",
      "Epoch 7600, Loss 2497571.75\n",
      "Epoch 7700, Loss 2498559.5\n",
      "Epoch 7800, Loss 2499559.5\n",
      "Epoch 7900, Loss 2500572.0\n",
      "Epoch 8000, Loss 2501597.5\n",
      "Epoch 8100, Loss 2502635.75\n",
      "Epoch 8200, Loss 2503688.75\n",
      "Epoch 8300, Loss 2502711.25\n",
      "Epoch 8400, Loss 2499538.75\n",
      "Epoch 8500, Loss 2495727.0\n",
      "Epoch 8600, Loss 2496564.0\n",
      "Epoch 8700, Loss 2493726.0\n",
      "Epoch 8800, Loss 2491625.0\n",
      "Epoch 8900, Loss 2490822.0\n",
      "Epoch 9000, Loss 2491605.0\n",
      "Epoch 9100, Loss 2490558.5\n",
      "Epoch 9200, Loss 2490663.0\n",
      "Epoch 9300, Loss 2490773.0\n",
      "Epoch 9400, Loss 2490888.75\n",
      "Epoch 9500, Loss 2491011.0\n",
      "Epoch 9600, Loss 2491139.5\n",
      "Epoch 9700, Loss 2491275.0\n",
      "Epoch 9800, Loss 2491418.25\n",
      "Epoch 9900, Loss 2491568.0\n",
      "Epoch 10000, Loss 2491725.0\n",
      "Epoch 10100, Loss 2491889.5\n",
      "Epoch 10200, Loss 2492062.25\n",
      "Epoch 10300, Loss 2492243.75\n",
      "Epoch 10400, Loss 2492435.0\n",
      "Epoch 10500, Loss 2492397.25\n",
      "Epoch 10600, Loss 2491546.75\n",
      "Epoch 10700, Loss 2491723.5\n",
      "Epoch 10800, Loss 2491909.5\n",
      "Epoch 10900, Loss 2492106.0\n",
      "Epoch 11000, Loss 2492312.5\n",
      "Epoch 11100, Loss 2492529.0\n",
      "Epoch 11200, Loss 2492757.5\n",
      "Epoch 11300, Loss 2492999.0\n",
      "Epoch 11400, Loss 2493251.5\n",
      "Epoch 11500, Loss 2493517.75\n",
      "Epoch 11600, Loss 2493798.0\n",
      "Epoch 11700, Loss 2494092.75\n",
      "Epoch 11800, Loss 2494402.0\n",
      "Epoch 11900, Loss 2492242.75\n",
      "Epoch 12000, Loss 2491177.75\n",
      "Epoch 12100, Loss 2491118.75\n",
      "Epoch 12200, Loss 2491279.25\n",
      "Epoch 12300, Loss 2491447.75\n",
      "Epoch 12400, Loss 2491625.0\n",
      "Epoch 12500, Loss 2491810.0\n",
      "Epoch 12600, Loss 2492006.0\n",
      "Epoch 12700, Loss 2492211.5\n",
      "Epoch 12800, Loss 2492427.25\n",
      "Epoch 12900, Loss 2492654.5\n",
      "Epoch 13000, Loss 2490882.0\n",
      "Epoch 13100, Loss 2490478.25\n",
      "Epoch 13200, Loss 2490601.5\n",
      "Epoch 13300, Loss 2490731.0\n",
      "Epoch 13400, Loss 2490867.25\n",
      "Epoch 13500, Loss 2491011.0\n",
      "Epoch 13600, Loss 2491162.0\n",
      "Epoch 13700, Loss 2491320.5\n",
      "Epoch 13800, Loss 2491486.75\n",
      "Epoch 13900, Loss 2491662.0\n",
      "Epoch 14000, Loss 2491845.0\n",
      "Epoch 14100, Loss 2492037.75\n",
      "Epoch 14200, Loss 2492241.5\n",
      "Epoch 14300, Loss 2492454.25\n",
      "Epoch 14400, Loss 2492678.75\n",
      "Epoch 14500, Loss 2492915.25\n",
      "Epoch 14600, Loss 2493163.75\n",
      "Epoch 14700, Loss 2493425.0\n",
      "Epoch 14800, Loss 2493700.0\n",
      "Epoch 14900, Loss 2493988.5\n",
      "Epoch 15000, Loss 2492905.75\n",
      "Epoch 15100, Loss 2493143.75\n",
      "Epoch 15200, Loss 2493395.5\n",
      "Epoch 15300, Loss 2493660.0\n",
      "Epoch 15400, Loss 2493938.25\n",
      "Epoch 15500, Loss 2494230.5\n",
      "Epoch 15600, Loss 2494537.75\n",
      "Epoch 15700, Loss 2494861.0\n",
      "Epoch 15800, Loss 2495200.0\n",
      "Epoch 15900, Loss 2495556.25\n",
      "Epoch 16000, Loss 2494545.5\n",
      "Epoch 16100, Loss 2494858.0\n",
      "Epoch 16200, Loss 2495187.75\n",
      "Epoch 16300, Loss 2495534.5\n",
      "Epoch 16400, Loss 2495898.75\n",
      "Epoch 16500, Loss 2496282.25\n",
      "Epoch 16600, Loss 2496685.5\n",
      "Epoch 16700, Loss 2497109.5\n",
      "Epoch 16800, Loss 2497555.5\n",
      "Epoch 16900, Loss 2498025.0\n",
      "Epoch 17000, Loss 2498519.0\n",
      "Epoch 17100, Loss 2499037.75\n",
      "Epoch 17200, Loss 2499583.25\n",
      "Epoch 17300, Loss 2500157.0\n",
      "Epoch 17400, Loss 2500759.75\n",
      "Epoch 17500, Loss 2501393.5\n",
      "Epoch 17600, Loss 2502059.75\n",
      "Epoch 17700, Loss 2502760.0\n",
      "Epoch 17800, Loss 2503496.5\n",
      "Epoch 17900, Loss 2500206.75\n",
      "Epoch 18000, Loss 2500858.5\n",
      "Epoch 18100, Loss 2500333.0\n",
      "Epoch 18200, Loss 2500895.5\n",
      "Epoch 18300, Loss 2501585.5\n",
      "Epoch 18400, Loss 2502310.75\n",
      "Epoch 18500, Loss 2503073.0\n",
      "Epoch 18600, Loss 2503875.25\n",
      "Epoch 18700, Loss 2504718.75\n",
      "Epoch 18800, Loss 2502648.75\n",
      "Epoch 18900, Loss 2503208.25\n",
      "Epoch 19000, Loss 2501297.5\n",
      "Epoch 19100, Loss 2500707.5\n",
      "Epoch 19200, Loss 2497384.0\n",
      "Epoch 19300, Loss 2497863.0\n",
      "Epoch 19400, Loss 2498473.0\n",
      "Epoch 19500, Loss 2496391.5\n",
      "Epoch 19600, Loss 2496928.25\n",
      "Epoch 19700, Loss 2495114.5\n",
      "Epoch 19800, Loss 2495590.0\n",
      "Epoch 19900, Loss 2496090.5\n",
      "Epoch 20000, Loss 2496617.25\n",
      "Epoch 20100, Loss 2497171.5\n",
      "Epoch 20200, Loss 2495994.25\n",
      "Epoch 20300, Loss 2496331.0\n",
      "Epoch 20400, Loss 2496865.25\n",
      "Epoch 20500, Loss 2494135.25\n",
      "Epoch 20600, Loss 2492001.0\n",
      "Epoch 20700, Loss 2491137.5\n",
      "Epoch 20800, Loss 2491544.75\n",
      "Epoch 20900, Loss 2491972.75\n",
      "Epoch 21000, Loss 2490219.0\n",
      "Epoch 21100, Loss 2485808.0\n",
      "Epoch 21200, Loss 2485987.0\n",
      "Epoch 21300, Loss 2485009.0\n",
      "Epoch 21400, Loss 2485237.0\n",
      "Epoch 21500, Loss 2485476.5\n",
      "Epoch 21600, Loss 2483446.0\n",
      "Epoch 21700, Loss 2483581.0\n",
      "Epoch 21800, Loss 2481312.75\n",
      "Epoch 21900, Loss 2481414.0\n",
      "Epoch 22000, Loss 2481521.0\n",
      "Epoch 22100, Loss 2481633.5\n",
      "Epoch 22200, Loss 2481752.0\n",
      "Epoch 22300, Loss 2481876.25\n",
      "Epoch 22400, Loss 2482006.75\n",
      "Epoch 22500, Loss 2482143.5\n",
      "Epoch 22600, Loss 2482287.0\n",
      "Epoch 22700, Loss 2482437.5\n",
      "Epoch 22800, Loss 2482596.0\n",
      "Epoch 22900, Loss 2482762.5\n",
      "Epoch 23000, Loss 2482938.0\n",
      "Epoch 23100, Loss 2483122.5\n",
      "Epoch 23200, Loss 2483316.0\n",
      "Epoch 23300, Loss 2483519.0\n",
      "Epoch 23400, Loss 2483733.0\n",
      "Epoch 23500, Loss 2483958.0\n",
      "Epoch 23600, Loss 2484194.75\n",
      "Epoch 23700, Loss 2483539.5\n",
      "Epoch 23800, Loss 2483382.0\n",
      "Epoch 23900, Loss 2483591.0\n",
      "Epoch 24000, Loss 2483810.25\n",
      "Epoch 24100, Loss 2484041.25\n",
      "Epoch 24200, Loss 2484284.0\n",
      "Epoch 24300, Loss 2484539.0\n",
      "Epoch 24400, Loss 2484807.5\n",
      "Epoch 24500, Loss 2485089.25\n",
      "Epoch 24600, Loss 2485385.25\n",
      "Epoch 24700, Loss 2485696.5\n",
      "Epoch 24800, Loss 2486023.0\n",
      "Epoch 24900, Loss 2486366.5\n",
      "Epoch 25000, Loss 2486728.0\n",
      "Epoch 25100, Loss 2484458.5\n",
      "Epoch 25200, Loss 2484638.0\n",
      "Epoch 25300, Loss 2484897.0\n",
      "Epoch 25400, Loss 2483850.5\n",
      "Epoch 25500, Loss 2484063.0\n",
      "Epoch 25600, Loss 2484286.75\n",
      "Epoch 25700, Loss 2484522.25\n",
      "Epoch 25800, Loss 2484769.25\n",
      "Epoch 25900, Loss 2485028.75\n",
      "Epoch 26000, Loss 2485302.25\n",
      "Epoch 26100, Loss 2485589.75\n",
      "Epoch 26200, Loss 2485892.0\n",
      "Epoch 26300, Loss 2486210.0\n",
      "Epoch 26400, Loss 2483888.0\n",
      "Epoch 26500, Loss 2484078.75\n",
      "Epoch 26600, Loss 2484279.0\n",
      "Epoch 26700, Loss 2484489.5\n",
      "Epoch 26800, Loss 2484710.75\n",
      "Epoch 26900, Loss 2484943.5\n",
      "Epoch 27000, Loss 2485188.25\n",
      "Epoch 27100, Loss 2485445.5\n",
      "Epoch 27200, Loss 2485715.5\n",
      "Epoch 27300, Loss 2485999.5\n",
      "Epoch 27400, Loss 2486298.0\n",
      "Epoch 27500, Loss 2486612.5\n",
      "Epoch 27600, Loss 2486943.0\n",
      "Epoch 27700, Loss 2487290.75\n",
      "Epoch 27800, Loss 2486173.5\n",
      "Epoch 27900, Loss 2486539.5\n",
      "Epoch 28000, Loss 2486924.5\n",
      "Epoch 28100, Loss 2485924.0\n",
      "Epoch 28200, Loss 2486255.5\n",
      "Epoch 28300, Loss 2486603.5\n",
      "Epoch 28400, Loss 2486969.5\n",
      "Epoch 28500, Loss 2487354.0\n",
      "Epoch 28600, Loss 2487758.0\n",
      "Epoch 28700, Loss 2488183.0\n",
      "Epoch 28800, Loss 2488629.25\n",
      "Epoch 28900, Loss 2487679.0\n",
      "Epoch 29000, Loss 2488067.5\n",
      "Epoch 29100, Loss 2488475.75\n",
      "Epoch 29200, Loss 2488905.0\n",
      "Epoch 29300, Loss 2489356.0\n",
      "Epoch 29400, Loss 2489830.0\n",
      "Epoch 29500, Loss 2490328.5\n",
      "Epoch 29600, Loss 2490852.5\n",
      "Epoch 29700, Loss 2491403.75\n",
      "Epoch 29800, Loss 2491983.0\n",
      "Epoch 29900, Loss 2492592.25\n",
      "Epoch 30000, Loss 2493232.5\n",
      "Epoch 30100, Loss 2492567.0\n",
      "Epoch 30200, Loss 2493135.25\n",
      "Epoch 30300, Loss 2493764.0\n",
      "Epoch 30400, Loss 2494425.0\n",
      "Epoch 30500, Loss 2495120.5\n",
      "Epoch 30600, Loss 2495851.25\n",
      "Epoch 30700, Loss 2496619.5\n",
      "Epoch 30800, Loss 2497427.25\n",
      "Epoch 30900, Loss 2498276.25\n",
      "Epoch 31000, Loss 2499168.5\n",
      "Epoch 31100, Loss 2500106.75\n",
      "Epoch 31200, Loss 2498510.5\n",
      "Epoch 31300, Loss 2495444.25\n",
      "Epoch 31400, Loss 2494850.5\n",
      "Epoch 31500, Loss 2495496.5\n",
      "Epoch 31600, Loss 2496186.5\n",
      "Epoch 31700, Loss 2496911.75\n",
      "Epoch 31800, Loss 2495613.75\n",
      "Epoch 31900, Loss 2490972.0\n",
      "Epoch 32000, Loss 2489906.0\n",
      "Epoch 32100, Loss 2490209.75\n",
      "Epoch 32200, Loss 2490549.25\n",
      "Epoch 32300, Loss 2490906.5\n",
      "Epoch 32400, Loss 2491283.0\n",
      "Epoch 32500, Loss 2487918.0\n",
      "Epoch 32600, Loss 2486880.75\n",
      "Epoch 32700, Loss 2486993.5\n",
      "Epoch 32800, Loss 2487119.5\n",
      "Epoch 32900, Loss 2487252.0\n",
      "Epoch 33000, Loss 2487391.25\n",
      "Epoch 33100, Loss 2487537.5\n",
      "Epoch 33200, Loss 2487690.75\n",
      "Epoch 33300, Loss 2487851.75\n",
      "Epoch 33400, Loss 2488021.25\n",
      "Epoch 33500, Loss 2488199.0\n",
      "Epoch 33600, Loss 2488385.75\n",
      "Epoch 33700, Loss 2488582.5\n",
      "Epoch 33800, Loss 2488789.75\n",
      "Epoch 33900, Loss 2489007.75\n",
      "Epoch 34000, Loss 2489237.5\n",
      "Epoch 34100, Loss 2489479.5\n",
      "Epoch 34200, Loss 2489734.0\n",
      "Epoch 34300, Loss 2490001.0\n",
      "Epoch 34400, Loss 2490281.5\n",
      "Epoch 34500, Loss 2489242.5\n",
      "Epoch 34600, Loss 2489469.25\n",
      "Epoch 34700, Loss 2489707.75\n",
      "Epoch 34800, Loss 2489958.5\n",
      "Epoch 34900, Loss 2490222.5\n",
      "Epoch 35000, Loss 2490499.75\n",
      "Epoch 35100, Loss 2490791.5\n",
      "Epoch 35200, Loss 2491097.0\n",
      "Epoch 35300, Loss 2491418.5\n",
      "Epoch 35400, Loss 2491756.5\n",
      "Epoch 35500, Loss 2492111.5\n",
      "Epoch 35600, Loss 2492484.5\n",
      "Epoch 35700, Loss 2490128.75\n",
      "Epoch 35800, Loss 2490359.0\n",
      "Epoch 35900, Loss 2490611.0\n",
      "Epoch 36000, Loss 2490876.5\n",
      "Epoch 36100, Loss 2489827.0\n",
      "Epoch 36200, Loss 2490038.5\n",
      "Epoch 36300, Loss 2490260.5\n",
      "Epoch 36400, Loss 2490494.25\n",
      "Epoch 36500, Loss 2490739.75\n",
      "Epoch 36600, Loss 2490998.0\n",
      "Epoch 36700, Loss 2491269.25\n",
      "Epoch 36800, Loss 2491554.5\n",
      "Epoch 36900, Loss 2491854.0\n",
      "Epoch 37000, Loss 2490279.5\n",
      "Epoch 37100, Loss 2489809.5\n",
      "Epoch 37200, Loss 2489991.25\n",
      "Epoch 37300, Loss 2490182.5\n",
      "Epoch 37400, Loss 2490382.75\n",
      "Epoch 37500, Loss 2490593.25\n",
      "Epoch 37600, Loss 2490814.25\n",
      "Epoch 37700, Loss 2491047.75\n",
      "Epoch 37800, Loss 2491292.25\n",
      "Epoch 37900, Loss 2491549.5\n",
      "Epoch 38000, Loss 2491820.0\n",
      "Epoch 38100, Loss 2492105.0\n",
      "Epoch 38200, Loss 2492404.25\n",
      "Epoch 38300, Loss 2492719.5\n",
      "Epoch 38400, Loss 2492229.0\n",
      "Epoch 38500, Loss 2491980.5\n",
      "Epoch 38600, Loss 2492265.0\n",
      "Epoch 38700, Loss 2492564.25\n",
      "Epoch 38800, Loss 2492878.5\n",
      "Epoch 38900, Loss 2493209.5\n",
      "Epoch 39000, Loss 2493556.75\n",
      "Epoch 39100, Loss 2493922.0\n",
      "Epoch 39200, Loss 2494052.25\n",
      "Epoch 39300, Loss 2493296.25\n",
      "Epoch 39400, Loss 2493638.25\n",
      "Epoch 39500, Loss 2493998.0\n",
      "Epoch 39600, Loss 2494376.25\n",
      "Epoch 39700, Loss 2494774.0\n",
      "Epoch 39800, Loss 2495192.0\n",
      "Epoch 39900, Loss 2495631.5\n",
      "Epoch 40000, Loss 2496093.0\n",
      "Epoch 40100, Loss 2496578.5\n",
      "Epoch 40200, Loss 2497088.75\n",
      "Epoch 40300, Loss 2497625.75\n",
      "Epoch 40400, Loss 2498190.25\n",
      "Epoch 40500, Loss 2497436.0\n",
      "Epoch 40600, Loss 2497961.75\n",
      "Epoch 40700, Loss 2498533.75\n",
      "Epoch 40800, Loss 2499135.75\n",
      "Epoch 40900, Loss 2496252.5\n",
      "Epoch 41000, Loss 2496355.75\n",
      "Epoch 41100, Loss 2496921.0\n",
      "Epoch 41200, Loss 2497515.5\n",
      "Epoch 41300, Loss 2498140.0\n",
      "Epoch 41400, Loss 2498796.5\n",
      "Epoch 41500, Loss 2499486.5\n",
      "Epoch 41600, Loss 2500211.75\n",
      "Epoch 41700, Loss 2500974.0\n",
      "Epoch 41800, Loss 2501775.75\n",
      "Epoch 41900, Loss 2501241.0\n",
      "Epoch 42000, Loss 2497722.5\n",
      "Epoch 42100, Loss 2497564.5\n",
      "Epoch 42200, Loss 2496874.75\n",
      "Epoch 42300, Loss 2497448.5\n",
      "Epoch 42400, Loss 2498051.5\n",
      "Epoch 42500, Loss 2498686.0\n",
      "Epoch 42600, Loss 2495743.0\n",
      "Epoch 42700, Loss 2358727.0\n",
      "Epoch 42800, Loss 2492311.0\n",
      "Epoch 42900, Loss 2491958.5\n",
      "Epoch 43000, Loss 2492253.0\n",
      "Epoch 43100, Loss 2492564.0\n",
      "Epoch 43200, Loss 2492890.75\n",
      "Epoch 43300, Loss 2492080.5\n",
      "Epoch 43400, Loss 2489850.75\n",
      "Epoch 43500, Loss 2489341.5\n",
      "Epoch 43600, Loss 2488949.0\n",
      "Epoch 43700, Loss 2489070.5\n",
      "Epoch 43800, Loss 2489197.5\n",
      "Epoch 43900, Loss 2489331.0\n",
      "Epoch 44000, Loss 2489471.5\n",
      "Epoch 44100, Loss 2489619.5\n",
      "Epoch 44200, Loss 2489775.0\n",
      "Epoch 44300, Loss 2489939.0\n",
      "Epoch 44400, Loss 2490112.0\n",
      "Epoch 44500, Loss 2490293.5\n",
      "Epoch 44600, Loss 2490484.25\n",
      "Epoch 44700, Loss 2490685.25\n",
      "Epoch 44800, Loss 2490897.0\n",
      "Epoch 44900, Loss 2491119.0\n",
      "Epoch 45000, Loss 2491352.5\n",
      "Epoch 45100, Loss 2491597.0\n",
      "Epoch 45200, Loss 2491854.75\n",
      "Epoch 45300, Loss 2490861.5\n",
      "Epoch 45400, Loss 2491055.5\n",
      "Epoch 45500, Loss 2491279.0\n",
      "Epoch 45600, Loss 2491514.0\n",
      "Epoch 45700, Loss 2491761.25\n",
      "Epoch 45800, Loss 2492020.5\n",
      "Epoch 45900, Loss 2492292.5\n",
      "Epoch 46000, Loss 2492578.75\n",
      "Epoch 46100, Loss 2492879.75\n",
      "Epoch 46200, Loss 2493196.0\n",
      "Epoch 46300, Loss 2493529.0\n",
      "Epoch 46400, Loss 2492037.0\n",
      "Epoch 46500, Loss 2491411.0\n",
      "Epoch 46600, Loss 2491637.5\n",
      "Epoch 46700, Loss 2491875.5\n",
      "Epoch 46800, Loss 2492125.5\n",
      "Epoch 46900, Loss 2491154.0\n",
      "Epoch 47000, Loss 2491314.5\n",
      "Epoch 47100, Loss 2491528.25\n",
      "Epoch 47200, Loss 2491753.75\n",
      "Epoch 47300, Loss 2491990.5\n",
      "Epoch 47400, Loss 2492239.5\n",
      "Epoch 47500, Loss 2492502.0\n",
      "Epoch 47600, Loss 2492777.75\n",
      "Epoch 47700, Loss 2493067.5\n",
      "Epoch 47800, Loss 2493372.5\n",
      "Epoch 47900, Loss 2491104.25\n",
      "Epoch 48000, Loss 2491284.75\n",
      "Epoch 48100, Loss 2491481.5\n",
      "Epoch 48200, Loss 2491688.0\n",
      "Epoch 48300, Loss 2491905.0\n",
      "Epoch 48400, Loss 2492133.5\n",
      "Epoch 48500, Loss 2492373.5\n",
      "Epoch 48600, Loss 2492625.75\n",
      "Epoch 48700, Loss 2492891.0\n",
      "Epoch 48800, Loss 2493169.75\n",
      "Epoch 48900, Loss 2493463.75\n",
      "Epoch 49000, Loss 2493772.25\n",
      "Epoch 49100, Loss 2493398.0\n",
      "Epoch 49200, Loss 2493020.5\n",
      "Epoch 49300, Loss 2493296.75\n",
      "Epoch 49400, Loss 2493587.5\n",
      "Epoch 49500, Loss 2493893.25\n",
      "Epoch 49600, Loss 2494214.5\n",
      "Epoch 49700, Loss 2494552.75\n",
      "Epoch 49800, Loss 2494907.75\n",
      "Epoch 49900, Loss 2493949.75\n"
     ]
    }
   ],
   "source": [
    "# 开始进行训练\n",
    "import torch\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "for epoch in range(50000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {-loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plant_ts = model.plant_ts.cpu().detach().numpy()\n",
    "plant_df = pd.DataFrame(plant_ts, columns=[crop_idx2name[i] for i in range(A_crop_len)], index=[land_idx2name[i] for i in range(A_land_len)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
